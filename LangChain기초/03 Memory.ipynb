{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c11133d-2e24-47be-b4fe-62a2414b15e4",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace16be5-09d5-46f8-802d-d9d7358d18ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f2d7aa-01e1-40c6-9e3c-2d12060d0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373766fd-e4a9-404e-9298-f47c19c21074",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "챗봇으로 하여금 대화(상태)를 '기억'하게끔 한다\n",
    "\n",
    "Memory maintains Chain state, incorporating context from past runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55072c28-113f-4dc4-89c9-aeb1e1c946ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain 의 memory 계층\n",
    "#  BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe891c52-3eaa-4ab2-b51b-b9f34fdc8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 사에서 제공하는 '기본 API' 도 랭체인 없이 사용 가능.\n",
    "# 메모리 지원하지 않는다.  이전 대화 기억 못함.  stateless 하다!\n",
    "\n",
    "# ChatGPT 서비스 는 '메모리' 기능이 탑재되어 있다.\n",
    "# 챗봇이 이전의 대화 내용과 질문을 기억하고 답할수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accc250-3065-43fa-82f9-2929a98ae6b3",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59615471-6702-47ce-90e0-c2281ec131b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferMemory\n",
    "# 대화 내용 '전체'를 저장하는 메모리\n",
    "\n",
    "# 장점: 단순하다\n",
    "\n",
    "# 단점:\n",
    "# => 매번 요청할때마다 '이전 대화 기록 전체' 를 같이 보내야 함.\n",
    "#  그래야 모델이 전에 일어났던 대화를 보고 이해 할수 있다.\n",
    "#  대화내용이 길어질수록 메모리도 계속 커지니까 성능적으로도 & 비용적으로도 비효율적이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f371a17a-da9e-431b-99e4-f04ba28f155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4442c352-ae62-4d69-83ef-93c73149b3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_19292\\3463026985.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi!\\nAI: How are you?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context(\n",
    "    {'input': 'Hi!'},    # 유저 입력값\n",
    "    {'output': 'How are you?'},  # 모델이 유저에게 답한값\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a0380-1622-4ab2-825c-883b2fc88b80",
   "metadata": {},
   "source": [
    "## return_messages=True\n",
    "history 에  AIMessage 와 HumanMessage 로 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eafa6370-4236-42fb-b2e2-d3b273f588f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), return_messages=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272a4487-c6df-44fa-8aff-c4c124e566e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {'input': 'Hi!'},    # 유저 입력값\n",
    "    {'output': 'How are you?'},  # 모델이 유저에게 답한값\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da99067-bed8-41c3-8128-5b18b78141f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {'input': 'Hi!'},    # 유저 입력값\n",
    "    {'output': 'How are you?'},  # 모델이 유저에게 답한값\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca550550-677b-42da-9a4c-a439b40c87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 반복하여 save 할수록 memory history 에 Message 들이 쌓인다.\n",
    "\n",
    "# 대화가 길어질수록 메모리에 내용이 계속 쌓인다 --> 비효율적."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ed145-e84e-42f2-9233-ee6e387068b2",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e6156c-7d37-4af4-a085-8920bba6d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.buffer_window import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50c344-fe88-4def-a8d9-d1e87d262661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferWindowMemory 는 대화의 '특정 부분만' 을 저장하는 메모리.\n",
    "\n",
    "# 장점:\n",
    "#   메모리를 특정 크기로 유지할 수 있다!\n",
    "#   따라서 모든 대화 내용을 저장하지 않아도 된다!\n",
    "\n",
    "# 단점:\n",
    "#   챗봇이 전체 대화가 아닌 '최근 대화' 에만 집중하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e7db10-7142-48e8-8d79-6ec47c3003ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_19292\\1088275114.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,  # 버퍼 윈도우 크기,  몇개의 context(메제지) 를 저장할지 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8a6e4b-f2a8-48c7-af4b-5c12b041d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도우미 함수, Message 추가하는 함수\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {'output': output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1a6217f-cf37-4271-b8e8-b313239f4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"1\", \"1\")\n",
    "add_message(\"2\", \"2\")\n",
    "add_message(\"3\", \"3\")\n",
    "add_message(\"4\", \"4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "799051b7-9cc8-4f58-a0e1-14510c7e081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "046cd5ed-45d5-435c-9e57-172216255c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='5', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='5', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"5\", \"5\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21484c82-af03-4c19-8f81-9d764cb3f1d1",
   "metadata": {},
   "source": [
    "# ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e43f2a-f3c4-4f7e-833d-0ee5005958ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationSummaryMemory 는 LLM 을 사용하여 대화의 요약본 (summary) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0f1821c-b10e-46e7-845c-1a1a1afdd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)  # 요약을 위해 모델 호출 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ab8dcb7-4add-49cd-a574-3e7a7ae888b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.summary import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af26016-fd79-4f4c-a860-8bdb11f3d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationSummaryMemory\n",
    "# 메세지를 그대로 저장하는 것이 아니라 Convertaion 의 '요약'을 해준다. LLM 필요\n",
    "\n",
    "# 장점: 대화의 메세지가 많아질수록 요약을 해주어 토큰의 양도 줄어들어 훨씬 경제적인 방법\n",
    "# 단점: LLM 호출 발생\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "595df82d-0810-4059-9e6d-5c746cb915d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_19292\\1565216213.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)  # LLM 사용\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm)  # LLM 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d5c828-34d1-4cf2-b5eb-ec999439f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도우미 함수\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cb7ccb4-6825-408a-b478-a5a97a6c51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message 추가\n",
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca006f7-27af-4e15-8495-3c853d318750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'John introduces himself as living in South Korea. The AI responds with enthusiasm, finding it cool.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b5f12-2dfd-41db-9a85-f019f6dc60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 대화를 '요약' 한 내용으로 기억하고 있다\n",
    "# 대화의 turn 이 길어질수로 summary 가 각 메세지를 효율적으료 '요약(압축)' 해준다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496b23a-7d64-4e1f-b731-e4bdd23d9da3",
   "metadata": {},
   "source": [
    "# ConversationSummaryBufferMemory\n",
    "- summry + buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "428f0b38-7a6c-4924-958c-1a01be5a68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.summary_buffer import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4b9f345-69e4-487a-ad9f-069beeee4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationSummaryBufferMemory 는\n",
    "#   ConversationBufferMemory 와 ConversationSummaryMemory 의 결합형\n",
    "\n",
    "# 메모리에 보내온 '메세지의 분량'를 지정하여 저장한다.\n",
    "# 오래된 메세지들 또한 '요약' 하여 저장함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65dd2a7d-201b-42f2-a46f-bcadc3486394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_19292\\2618822315.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,  # 최대 가용한 메세지 토큰수 (메세지들이 요약되기전)\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04f0d923-832c-42bc-b19d-b3d252322e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aabf39c6-0493-4d09-b972-212ada174548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다시 메세지 추가하고 history 확인\n",
    "add_message(\n",
    "    \"South Korea is so pretty\",\n",
    "    \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7bb14-c4ee-412c-8669-b3cd0c23fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아직 요약은 발생하지 않았다.  max_token_limit=150 이하이기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f10459b-c4ce-4396-9a09-77e39ebd07ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Korea from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"How far is Korea from Argentina?\",\n",
    "    \"I don't know! Super far!\"\n",
    ")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1255137e-f771-4ed6-bfa1-2e1921300808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces himself as John and mentions that he lives in South Korea.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Korea from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"How far is Brazil from Argentina?\",\n",
    "    \"I don't know! Super far!\"\n",
    ")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30000ca2-389d-480a-a945-796123c7f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_token_limit 에 도달하면, 오래된 메세지들이 요약되고 있을 것을 확인할수 있다. (SystemMessage 확인)\n",
    "\n",
    "# ★그러나 '요약' 이라는 과정은 API 를 사용한다는 사실을 명심하세요.\n",
    "# ★'요약' 동작은 비용 지출이 발생되는 부분입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f42b3-fd1a-4345-8a2d-ad2f63324c33",
   "metadata": {},
   "source": [
    "# ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e985094f-5935-448c-ace4-ae0d99f51990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KG : Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b43cd9a-e248-4c0a-816c-ffeb4ea946eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.memory.kg import ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee707247-71ec-4ea2-984b-ec9b54a96f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화중에 '엔티티'의 knowledge graph 를 형성한다 => 가장 중요한 것들만 추출한 요약본.\n",
    "# knowledge graph 는 history 를 가지고 오지 않는다.  대신 '엔티티' 를 가지고 옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4c48f01-7c86-4d68-b773-8fa1694d7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationKGMemory(\n",
    "    llm=llm,  # 이 또한 LLM 을 사용하는 Memory 다 -> Knowledge Graph 를 만든다. (가장 중요한것만 뽑아낸 요약본)\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7836d046-bb2a-4238-9a0a-6d697e66e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b512b54e-2042-49dd-a3fa-245092ed285a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On John: John lives in South Korea.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대화의 특정 entity 에 대해 질문할수 있다.\n",
    "memory.load_memory_variables({'input': 'who is john'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "307fe68c-2372-44ee-9f11-1f4480df23d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On John: John lives in South Korea. John likes kimchi.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"John likes kimchi\", \"Wow that is so cool!\")\n",
    "\n",
    "memory.load_memory_variables({\"inputs\": \"What does John like\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d136c-2a5c-4844-8b14-49dc5ecb83ff",
   "metadata": {},
   "source": [
    "# 그 밖의 메모리들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52723d1d-632b-4040-9c0e-8e906f27547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationTokenBufferMemory 도 있다\n",
    "#   interaction 의 최대값을 가지고 있는 것 대신에 token의 총 양을 계산하는게 전부다.\n",
    "#   max_token_limit= 값!  <= ConversationBufferWindowMemory 와 비슷하다 (k= 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf63885-57b2-465c-b9f7-15236df86d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationEntityMemory\n",
    "#  Entity 를 활용한 메모리도 있다.\n",
    "#  이는 대화중에 entity 를 추출해 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52308e1c-9af1-436d-9077-1d75c5da767b",
   "metadata": {},
   "source": [
    "- **참고: Database 와 integration 된 메모리들**\n",
    "\n",
    "| Memory Class                  | 통합 대상 (Integration)             | 설명                                                           |\n",
    "| ----------------------------- | ------------------------------- | ------------------------------------------------------------ |\n",
    "| `RedisChatMessageHistory`     | **Redis**                       | Redis에 메시지 저장. 빠르고 확장 가능한 저장소.                               |\n",
    "| `SQLChatMessageHistory`       | **SQLite, PostgreSQL 등 SQL DB** | SQL 데이터베이스에 메시지 저장. SQLAlchemy 기반.                           |\n",
    "| `DynamoDBChatMessageHistory`  | **AWS DynamoDB**                | AWS의 NoSQL DB인 DynamoDB에 대화 저장. 서버리스 환경에서 유용.                |\n",
    "| `MongoDBChatMessageHistory`   | **MongoDB**                     | 문서 기반 DB인 MongoDB와 통합하여 대화 저장.                               |\n",
    "| `PostgresChatMessageHistory`  | **PostgreSQL**                  | PostgreSQL 전용 구현 (SQLAlchemy 없이).                            |\n",
    "| `FileChatMessageHistory`      | **Local 파일**                    | JSON 파일로 로컬에 저장. 간단한 로깅에 적합.                                 |\n",
    "| `FirestoreChatMessageHistory` | **Google Firestore**            | Firebase 기반 클라우드 NoSQL DB와 통합.                               |\n",
    "| `ChromaMemory`                | **Chroma (벡터 DB)**              | 벡터 DB에 embedding 형태로 memory 저장. RAG나 유사 검색 기반 memory에 활용 가능. |\n",
    "| `WeaviateMemory`              | **Weaviate**                    | Weaviate 벡터 DB와 통합된 memory 저장.                               |\n",
    "| `QdrantMemory`                | **Qdrant**                      | 벡터 기반 memory 저장소로 Qdrant 사용.                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2b7a0-5c9a-4c84-b7b7-0d99d2ceba9e",
   "metadata": {},
   "source": [
    "# Memory on LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acfb25-d1fe-44a6-996b-2540a27e09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. off-the-shelf chain : LangChain 측에서 '일반적인 목적'을 수행하는 기본제공 chain\n",
    "# 2. LCEL 사용한 chain : 커스컴 chain\n",
    "\n",
    "# 위 두가지 형태의 chain 에 각각 메모리를 꽂아 사용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0b73058-0de4-4716-8d8f-241035b3f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# off-the-shelf chain 하나를 사용해보자.\n",
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd24ae0-02b1-4fad-8b50-cbd08f03693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMChain\n",
    "#  => 'prompt | llm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2791922-e985-4329-8141-618159cf6d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_19292\\3255347543.py:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit=80,\n",
    ")\n",
    "\n",
    "# chain 생성하고 메모리 장착!\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,  # 메모리 제공!\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19c8f651-56ec-4b0c-ac85-f561150aadd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'history': '',\n",
       " 'text': 'Nice to meet you, John! How can I assist you today?'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': \"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef19b4-f4de-4acc-af6e-4a0cf8f5ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'My name is John',\n",
    " 'history': '',    # <- 최초에는 history 가 비어있다.\n",
    " 'text': 'Nice to meet you, John! How can I assist you today?'}\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09c67a17-9f76-4606-aaa7-8a7ac3f47896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
       " 'text': \", the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views from Namsan Mountain. Seoul is a dynamic city that never fails to impress me with its energy and charm.\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': \"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75766d55-509c-4df8-83ab-798108a0eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'I live in Seoul',\n",
    "    ↓ 이전 대화에 대한 history 가 생겼다! <- memory 동작한다는 뜻!\n",
    " 'history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
    " 'text': \", the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views from Namsan Mountain. Seoul is a dynamic city that never fails to impress me with its energy and charm.\"}\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dea87318-5f69-4c34-bbf7-713c2ce4513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name',\n",
       " 'history': \"System: The human introduces himself as John and mentions he lives in Seoul, the capital city of South Korea. The AI responds by describing Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. The AI expresses its love for exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views from Namsan Mountain, highlighting Seoul's dynamic energy and charm.\",\n",
       " 'text': \"I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과연 기억하고 있을까?\n",
    "chain.invoke({'question': \"What is my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1cf464-da0a-404d-b256-c0f998137173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'What is my name',\n",
    " 'history': \"System: The human introduces himself as John and mentions he lives in Seoul, the capital city of South Korea. The AI responds by describing Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. The AI expresses its love for exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views from Namsan Mountain, highlighting Seoul's dynamic energy and charm.\",\n",
    " 'text': \"I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\"}\n",
    "\n",
    "어라?  모른다고?\n",
    "\n",
    "chain 디버깅을 통해 알아보자.\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df488a4-c834-4bb3-b8db-f71a611330c2",
   "metadata": {},
   "source": [
    "## verbose= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6349509f-5182-4f65-8791-13c9c11ee7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mMy name is John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'history': \"System: The human, named John, mentions he lives in Seoul, the capital city of South Korea. The AI responds by describing Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. The AI expresses its love for exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views from Namsan Mountain, highlighting Seoul's dynamic energy and charm. John confirms his name and location, prompting the AI to greet him and offer assistance.\\nAI: , the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic temples. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River. Seoul is a dynamic and exciting place to call home.\",\n",
       " 'text': \"I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,  # 메모리 제공!\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True,  # chain 을 실행했을때 chain 의 프롬프트 로그들 확인 가능. (디버깅용)\n",
    ")\n",
    "\n",
    "chain.invoke(input={'question':\"My name is John\"})\n",
    "chain.invoke(input={'question':\"I live in Seoul\"})\n",
    "chain.invoke(input={'question':\"What is my name?\"})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f410b-c742-4daf-9651-9512c0ba4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "My name is John  <----  이게 프롬프트가 가진 전부다\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "I live in Seoul  <----  이게 프롬프트가 가진 전부다\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "What is my name?  <----  이게 프롬프트가 가진 전부다\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\n",
    "↑ chain 의 prompt 로그 들을 확인할 수 있다.\n",
    "보다시피 대화의 내역(history) 가 prompt에 계속 추가되진 않는다.\n",
    "\n",
    "우리가 원하는 어떤 방식으로 prompt에게 대화 기록(history)을 추가해줘야 한다!\n",
    "\n",
    "↓ 하지만! 메모리는 계속 업데이트 된다.  함 보자!\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51240467-029e-4c14-810c-c87f2b9f7794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: John mentions he lives in Seoul, the capital city of South Korea. The AI responds by describing Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic temples. The AI expresses its love for exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River, highlighting Seoul's dynamic energy and charm. John confirms his name and location, prompting the AI to greet him and offer assistance.\\nHuman: What is my name?\\nAI: I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8422e42-1a59-42dd-8b7f-d27fff09e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 메모리 내용이 prompt 에 포함되어야 한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f18e1-ee97-47da-8f43-523b93b729b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory 클래스에게\n",
    "# 템플릿 안에 콘텐츠를 넣으라고 해주어야 한다.\n",
    "# memory_key=  에 \"chat_history\" (예시)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126302d7-d0fc-4a95-b125-25cf686d7a11",
   "metadata": {},
   "source": [
    "## memory_key="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49c2cd66-6920-42df-ae75-d8a25efa3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120, \n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# ↓ template 안에는 memory 가 history 를 저장하도록 한 곳을 적어주기만 하면 된다\n",
    "# history 까지 담을 괜찮은 템플릿을 준비해보자\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9520c986-8c34-49e2-ae9a-3e8db35a50cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is John\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'chat_history': '',\n",
       " 'text': 'Nice to meet you, John! How can I assist you today?'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': \"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5debb-e8b9-4f53-8e67-2ddb8147ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {chat_history} 에는 아무 내용도 포맷팅 안됨 <- 메모리 비어 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c88ff8a9-b316-4ffb-8762-bcc11230944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
       " 'text': \"That's great to hear! How can I assist you with information or tasks related to Seoul?\"}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': \"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313461a-f8b7-422b-bea9-beb88e57c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ {chat_history} 에 메모리의 저장된 이전 대화 내용이 포맷팅 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4094578-3839-4dad-b886-fd7d80243621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'chat_history': \"Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\",\n",
       " 'text': 'Your name is John.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'What is my name?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bd750-96fe-4733-b92d-4fde4f5c14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 프롬프트 템플릿 안에서 메모리 내용이 들어갈 공간을 준비한다.  (예: chat_history)\n",
    "# - 메모리를 활용할 템플릿은 원하는대로 작성하면 된다.\n",
    "# - Memory 클래스에선 history 를 어디에 꽂을지 지정해준다 (memory_key=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30e3d6c2-c439-4faf-9b9f-a8ddcfd5cbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: The human introduces himself as John. The AI responds by saying, \"Nice to meet you, John! How can I assist you today?\"\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is John.\n",
      "Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "    Human:My name is John\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: John introduces himself to the AI and mentions that he lives in Seoul. The AI responds by offering assistance with information or tasks related to Seoul.\n",
      "Human: What is my name?\n",
      "AI: Your name is John.\n",
      "Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': \"System: John introduces himself to the AI and mentions that he lives in Seoul. The AI responds by offering assistance with information or tasks related to Seoul.\\nHuman: What is my name?\\nAI: Your name is John.\\nHuman: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\\nHuman: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\",\n",
       " 'text': \"That's great to hear! How can I assist you with information or tasks related to Seoul?\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과연 요약은 잘 될까?\n",
    "chain.invoke({'question': \"My name is John\"})\n",
    "chain.invoke({'question': \"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa7e399c-847a-413f-b449-18084af2cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금까지는 텍스트 기반의 history 였다.\n",
    "\n",
    "# 과연 Message 기반의 history 는 어떻게 적용하나?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05899bf1-f4ec-4036-8c12-6b0069bb53b1",
   "metadata": {},
   "source": [
    "# Chat Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcd3af-efab-4359-ba9f-ff504be419f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory 클래스는 memory 를 두가지 방식으로 출력할수 있다는 사실을 기억하자..\n",
    "# 1. 문자열 형태\n",
    "# 2. message 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "596d4238-5325-445a-a0d7-26e8bd182113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"System: John introduces himself to the AI and mentions that he lives in Seoul. The AI responds by offering assistance with information or tasks related to Seoul. When asked, the AI correctly identifies John's name as John.\\nHuman: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\\nHuman: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "# ↓ 직전의 메모리는 '텍스트 기반'의 메세지 였다.  이 텍스트가 그대로 프롬프트에 포맷팅 된거다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecff1b-22de-4774-ad16-83995fb1d911",
   "metadata": {},
   "source": [
    "## return_meesages=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f950c190-f83f-47c3-80d7-7817e82e5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이를 대화 기반의 채팅으로 사용하고 싶다면 변경해야 한다\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,   # history 가 문자열기반이 아닌 Message 로 리턴됨.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1be7c-cadf-4cb2-8cf2-69b9fa901a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과연 prompt 에는 어떻게 대화 history 들을 넘겨줄수 있을까?\n",
    "#  단순한 하나의 텍스트가 아니라... Human Message - AI Message - Human Messagbe - AI Message - .... (여러 메세지들)\n",
    "#  심지여 요약본 발생시 System message 도 있을텐데?\n",
    "\n",
    "# prompt 에 이를 위한 공간을 어케 만드나?\n",
    "#  => MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba3145-5e59-428f-a731-00803f1f67d0",
   "metadata": {},
   "source": [
    "## MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "214143a7-7c2f-4d12-9d37-dc1af8dd81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4eb24c2-8f50-4edf-8115-05c502bb3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        # ↑ variable_name=\"chat_history\"\n",
    "        #  ConversationSummaryBufferMemory 는\n",
    "        #    history 에서 message 들을 가져와서 (return_messages=True)\n",
    "        #    이곳 MessagesPlaceHolder 를 채운다.\n",
    "        #     AI message, System message, Human message ...\n",
    "        #     얼마나 많은지 알수 없어도 여기에 채워지는 거다!    \n",
    "    \n",
    "        (\"human\", \"{question}\"),    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2925631c-2d00-4960-8995-9516de141459",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad450945-5ab3-4c59-aeaf-fb74b0c51660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Hello John! How can I assist you today?'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d021ce4-581d-43c3-aafe-b799fb73dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "System: You are a helpful AI talking to a human   <-- 'system' message\n",
    "                                                <-- MessagesPlaceholder 에는 아직 history 없다.\n",
    "Human: My name is John                         <-- 'human' message\n",
    "\n",
    "> Finished chain.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcfdd076-f3af-451a-8361-3289bf5ce2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I live in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a1de4-8ba5-4c9e-9803-aaa0eb91f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "System: You are a helpful AI talking to a human  <--- ★\n",
    "Human: My name is John                           <--- ★ MessagesPlaceHolder 의 history\n",
    "AI: Hello John! How can I assist you today?      <--- ★  MessagesPlaceHolder 의 history\n",
    "Human: I live in Seoul                           <--- ★\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f32b6f74-b9ec-407f-8563-58b7cfd9674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I live in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is John.', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Your name is John.'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f975c2c6-c8c9-4879-85e1-520c14a6e947",
   "metadata": {},
   "source": [
    "# LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcb9e5-ef2d-46d3-8a8f-4a7afa01f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동으로 만들어진 chain  에\n",
    "# langchain expression language(LCEL) 를 활용해서 memory 를 추가하는 방법을 배워보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd176f4a-95bd-499a-a970-dcb095331993",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e554530-5327-4920-a214-de8e96019e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CjOqv1O0WENszJFY0XkDZDP1cnJEL', 'finish_reason': 'stop', 'logprobs': None}, id='run--ccfc6cbc-b58b-4fce-8a22-d6434eda9f4b-0', usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"chat_history\": memory.load_memory_variables({})['chat_history'],\n",
    "    \"question\": 'My name is John'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad8aad-b0ca-42e4-9657-f2ea0c7e99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 위 방법도 가능은 하다...\n",
    "# 단, 위 접근 방식의 문제는\n",
    "# 우리가 chain 을 호출할 때마다 chat_history 도 추가해줘야 한다는 거다.\n",
    "\n",
    "# ↓ 이보다 더 좋은 방법도 있다. 바로 'Runnables' 라는 것을 사용하는 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a96f54-ba51-4cce-9df9-fd2045096407",
   "metadata": {},
   "source": [
    "## RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f41595b7-5f36-41bc-b7d1-28c59c609107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.passthrough import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9fa143d0-e510-467a-8a5a-17db15082f74",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_memory() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m memory.load_memory_variables({})[\u001b[33m'\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMy name is John\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:511\u001b[39m, in \u001b[36mRunnableAssign.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    509\u001b[39m     **kwargs: Any,\n\u001b[32m    510\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:497\u001b[39m, in \u001b[36mRunnableAssign._invoke\u001b[39m\u001b[34m(self, value, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    492\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    496\u001b[39m     **value,\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     **\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    502\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3774\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3769\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3770\u001b[39m         futures = [\n\u001b[32m   3771\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3772\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3773\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3775\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3758\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3752\u001b[39m child_config = patch_config(\n\u001b[32m   3753\u001b[39m     config,\n\u001b[32m   3754\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3755\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3756\u001b[39m )\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4771\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4757\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4758\u001b[39m \n\u001b[32m   4759\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4768\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4769\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4770\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4772\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4773\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4775\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4776\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4777\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4778\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4629\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4627\u001b[39m                 output = chunk\n\u001b[32m   4628\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4629\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4630\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4632\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2507\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: load_memory() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# 메모리 변수 값 리턴하는 함수\n",
    "def load_memory():\n",
    "    return memory.load_memory_variables({})['chat_history']\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "chain.invoke({\"question\": \"My name is John\"})\n",
    "\n",
    "\"\"\"\n",
    "↑ 위를 실행하게 되면 가장 먼저 load_memory() 를 호출한다.\n",
    "prompt가 필요로 하는 chat_history= 키 내부에 넣는다.\n",
    "\n",
    "이는 마치\n",
    "chain.invoke({\n",
    "    \"chat_history\": load_memory(),  <- 요렇게 한것과 동일하다.\n",
    "    \"question\": \"My name is John\",\n",
    "})\n",
    "\n",
    "prompt 가 포맷팅 되기 전에 load_memory() 함수가 실행되는 것을 허락해준다.\n",
    "\n",
    "\n",
    "↓ 그런데 실행하면 에러 날거다!\n",
    "\n",
    "TypeError: load_memory() takes 0 positional arguments but 1 was given\n",
    "\n",
    "호출되는 함수 (load_memory) 에 한개의 input 값이 전달되기 때문에 그렇다.\n",
    "\n",
    "↓ 도대체 어떤값이 load_memory() 에 전달되는지 확인해보자!\n",
    "\"\"\"\n",
    "None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fd34402-9d5a-4fe7-b2b0-050304ae8f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😎load_memory(): {'question': 'My name is John'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CjP1gYV2DmIX4BqhB6c4mV7KV0WuV', 'finish_reason': 'stop', 'logprobs': None}, id='run--8515dde3-0b35-44e1-aa12-b6249efb6ff5-0', usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 도대체 어떤값이 load_memory() 에 전달되는지 확인해보자!\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "    print(\"😎load_memory():\", input)  # 확인!\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"My name is John\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df89ed3-4ce7-4606-ae67-5c7ca6d0bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load_memory() {'question': 'My name is John'}\n",
    "\n",
    "체인에 있는 모든 컴포넌트는 input 을 받을거고, 또 output 을 줄거다.\n",
    "이게 랭체인의 핵심이다.  모든 것이 input을 얻을거고, 그 후엔 output 을 줄거다.\n",
    "\n",
    "\n",
    "chain.invoke(\n",
    "  {     <--  이 dict 는 chain 의 첫번째 아이템의 input 이 되는거다.\n",
    "             바로 그게 load_memory() 의 input 이 된것이다!   규칙이다!\n",
    "    \"question\": \"My name is John\",\n",
    "  }\n",
    ")\n",
    "\n",
    "이후 load_memory() 를 실행히킨 결과로 얻은것이 chat_history 속성으로 들어가\n",
    "체인의 다음 요소 로 전달되는 것이다.\n",
    "\n",
    "RunnablePassThrough 는 최초 전달받은 'question' 을 체인의 다음요소 까지 전달 (key 값 변경 없이!)\n",
    "\n",
    "\"\"\"\n",
    "None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6c12ce2b-7830-4599-bfd0-215a2d794a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 호출 결과를 다시 메모리에 저장하려면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6f1d85e-30fd-4556-b1bd-548fa6f17553",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "    print(\"😎load_memory():\", input)  # 확인!\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "# 체인 호출 함수를 만들어 보자\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question,\n",
    "    })\n",
    "    # 체인 호출 결과를 메모리에 저장\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "813564bc-4231-44d6-bc79-7bf262cfe5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😎load_memory(): {'question': 'My name is John'}\n",
      "content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CjP7bFKS7eI6XCEUHUYiiZJXwO2YI', 'finish_reason': 'stop', 'logprobs': None} id='run--8998807b-edaa-4c8a-befb-1effd0bdd5d8-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a657eea-4d9d-4519-b627-d51f1dd62900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😎load_memory(): {'question': 'What is my name?'}\n",
      "content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CjP8ERxLFgDkIREYB9BCj2dLFihhp', 'finish_reason': 'stop', 'logprobs': None} id='run--8c4b79e1-9943-4bca-a5d4-55ba1035e56c-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b8c500a-15c7-4e47-b1a0-cdd3c3f59347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is John.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745a647-475e-403f-bba3-6dbaa7297a21",
   "metadata": {},
   "source": [
    "## memory_key= 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1196042c-2068-43e8-bead-4df1b61d5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory_key= 의 기본값은 'history' 였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6062150-d3e7-4fb0-ba26-de58b479da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\",  # 삭제!\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # 변경!\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "    print(\"😎load_memory():\", input)  \n",
    "    return memory.load_memory_variables({})[\"history\"]  # 변경!\n",
    "\n",
    "                                  # ↓ 변경!\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question,\n",
    "    })\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43222bbb-556b-4bc2-bea7-52106a9c6629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😎load_memory(): {'question': 'My name is John'}\n",
      "content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CjPEO55j6ZFmbPlm2X0ffnJxupp0O', 'finish_reason': 'stop', 'logprobs': None} id='run--22cbc1f5-f153-4705-8d19-8001cb4ecdfb-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5f2d394-74eb-4fad-8614-16ecca9db2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😎load_memory(): {'question': 'What is my name?'}\n",
      "content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CjPEhdhsffc4TSH5FBrmwqurWVV8T', 'finish_reason': 'stop', 'logprobs': None} id='run--3bd25d90-d6da-4af9-8a22-2dd9dd2161ee-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e83348c-433b-4a46-a396-6408e4669d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CjPJBpNM9NgV75KNIIj8LV8gipcgX', 'finish_reason': 'stop', 'logprobs': None} id='run--91ea8bc0-ff0f-4678-aa4b-f7adade5838d-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CjPJCQRY7W6DZjefXg0xlNQIX9ug5', 'finish_reason': 'stop', 'logprobs': None} id='run--001c6e5d-6913-4d3b-ae2c-7f093ca80dae-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\",  # 삭제!\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # 변경!\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory():\n",
    "    return memory.load_memory_variables({})[\"history\"]  # 변경!\n",
    "        \n",
    "chain = prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"history\": load_memory(),\n",
    "        \"question\": question,\n",
    "    })\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "invoke_chain(\"My name is John\")\n",
    "invoke_chain(\"What is my name?\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ff904-6812-4403-8355-46f25a548a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01b17a-01e1-4961-98e5-5b2d45b1642d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7701d13-d83f-4d02-8cd0-73cdfa612035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7612e9-806b-4ad6-b5eb-e89ac3af3609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61fa33-eaa2-4942-84e7-b41061bedf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f3cf8-4753-4bd2-bbe7-3a87ffe6f5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025cba7d-d2a5-4d8b-802d-5146a034e67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f4e98-d074-431c-bc8c-7c50020dbad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fbbb57-935e-462a-a2ad-11f73654c96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd5fd5-7ffa-4bea-8692-026df4c0f157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023eb16-6b15-4c68-a7c3-3995ffacbf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8bfd1-1f39-464e-87fd-2bb6554805f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783537d-ee4b-4688-8ab3-c05efe679557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14475333-f353-4353-a740-13b3bf15dc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf759c-b785-436d-b23e-38e427e3f377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386efcb5-eaeb-4531-b4d2-d36854cec0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f10888-8fe4-4a7a-9911-a4e8ccca41d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bffca-4efe-4ade-93ee-a4e1e15dce49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b638459-3358-4741-8257-364e610d98f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80beab08-3517-4b10-834d-2e8ba01eae70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbe105-63da-482c-879f-b81e68b25f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8b3e6-b297-4578-acf1-8b2b57cc28d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368e44f-5750-411c-b3db-2554a3616303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f8825-2939-429e-9c8d-849b24284804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5a503-fa58-4bc8-95e4-9dff4698b366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40a692-a118-4060-9444-be1e72cdf0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7c27a-03c4-40c5-9f9e-db925d6d1aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5791b-b9b1-4b2d-9d5a-680f777ace37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92ba59-067a-4374-80ed-f0d008f24946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
