{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589915ca-bf3f-43e8-b2ff-afd0ec5f1911",
   "metadata": {},
   "source": [
    "# Hello LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7e95e-1b79-43ba-a699-1b2a4e375198",
   "metadata": {},
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0720a170-2ae0-43c6-bdd4-3034a2db221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-iKU13YeoxNgF...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "print(f'{os.environ['OPENAI_API_KEY'][:20]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ae117-4147-4d19-8519-334243356a0d",
   "metadata": {},
   "source": [
    "# LangChain  ê´€ë ¨ ì£¼ìš” ë§í¬\n",
    "\n",
    "-  Python Langchain ê³µì‹ í™ˆ:  https://python.langchain.com/\n",
    "-  API ë ˆí¼ëŸ°ìŠ¤ í™ˆ: https://python.langchain.com/api_reference/reference.html\n",
    "\n",
    "\n",
    "## Langchain ì˜ íŒ¨í‚¤ì§€ êµ¬ì„±\n",
    "\n",
    "\n",
    "### Base Packages\n",
    "- [Core: langchain-core](https://python.langchain.com/api_reference/core)\n",
    "- [Langchain: langchain](https://python.langchain.com/api_reference/langchain)\n",
    "- [Test Splitters: langchain-text-splitters](https://python.langchain.com/api_reference/text_splitters)\n",
    "- [Community: langchain-community](https://python.langchain.com/api_reference/community)\n",
    "- [Experimental: langchain-experimental](https://python.langchain.com/api_reference/experimental)\n",
    "\n",
    "### Integrations\n",
    "- ë­ì²´ì¸ì€ ìˆ˜ë§ì€ LLM ëª¨ë¸ë“¤ê³¼ ì»¤ë®¤ë‹ˆí‹°, ë²¡í„°ìŠ¤í† ì–´, ë°ì´í„°ë² ì´ìŠ¤, íˆ´ ë“¤ê³¼ í•¨ê»˜ ì‚¬ìš©í• ìˆ˜ ìˆë„ë¡ ì œê³µë˜ëŠ” íŒ¨í‚¤ì§€ë“¤ì´ ë§ë‹¤ (ì•ìœ¼ë¡œ ë” ë§ì•„ ì§ˆê±°ë‹¤)\n",
    "- [OpanAI: langchain-openai](https://python.langchain.com/api_reference/openai)\n",
    "- [Huggingface: langchain-huggingface](https://python.langchain.com/api_reference/huggingface)\n",
    "- [MistalAI: langchain-mistralai](https://python.langchain.com/api_reference/mistralai)\n",
    "- ê·¸ë°–ì—ë„ ë§ì´ ìˆë‹¤ ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65b38d-f4db-4ae2-983c-e9e96dbb2034",
   "metadata": {},
   "source": [
    "# â–  LLM vs. Chat model\n",
    "\n",
    "LangChain ì€ LLM ê³¼ Chat model ë‘ê°€ì§€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤\n",
    "\n",
    "`LLM`(Large Language Model)ê³¼ `Chat Model`ì€ ë¹„ìŠ·í•œ ì—­í• ì„ í•˜ì§€ë§Œ, ì•½ê°„ì˜ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŠ” ì£¼ë¡œ **ëª¨ë¸ì˜ ì…ë ¥ ë° ìƒí˜¸ì‘ìš© ë°©ì‹**ì—ì„œ ë‚˜íƒ€ë‚œë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. LLM (Large Language Model)\n",
    "- **íŠ¹ì§•**:\n",
    "  - ì¼ë°˜ì ìœ¼ë¡œ **í…ìŠ¤íŠ¸ ì…ë ¥**ì„ ë°›ê³ , ì´ì— ëŒ€í•œ **í…ìŠ¤íŠ¸ ì¶œë ¥**ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  - ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì…ë ¥/ì¶œë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "  - ì‚¬ìš©ìê°€ ì œê³µí•œ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³ , ê·¸ì— ëŒ€í•œ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "- **ì…ë ¥ í˜•ì‹**:\n",
    "  ```plaintext\n",
    "  \"Tell me a summary of the benefits of LangChain.\"\n",
    "  ```\n",
    "- **ì¶œë ¥ í˜•ì‹**:\n",
    "  ```plaintext\n",
    "  \"LangChain is a framework designed to simplify the development of applications powered by large language models, making it easier to manage prompts, chains, and integrations.\"\n",
    "  ```\n",
    "- **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€**:\n",
    "  - ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€\n",
    "  - í…ìŠ¤íŠ¸ ìƒì„±\n",
    "  - ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—…\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Chat Model\n",
    "- **íŠ¹ì§•**:\n",
    "  - **ëŒ€í™” í˜•ì‹**ìœ¼ë¡œ ì„¤ê³„ëœ ëª¨ë¸ë¡œ, ë‹¤ì¤‘ í„´ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.\n",
    "  - ì…ë ¥ í˜•ì‹ì´ **ë©”ì‹œì§€ Message**ë¡œ êµ¬ì„±ë˜ë©°, ê° ë©”ì‹œì§€ëŠ” ì‚¬ìš©ìì˜ ë©”ì‹œì§€ (User Message)ì™€ ì‹œìŠ¤í…œì˜ ë©”ì‹œì§€(System Message)ë¡œ ë‚˜ë‰œë‹¤.\n",
    "  - 'ë¬¸ë§¥'ì„ ì´í•´í•˜ê³  'ëŒ€í™”ì˜ íë¦„'ì„ ìœ ì§€í•˜ëŠ” ë° ìµœì í™”ë˜ì–´ ìˆë‹¤.\n",
    "- **ì…ë ¥ í˜•ì‹**:\n",
    "  ë©”ì‹œì§€ ê°ì²´ë¥¼ ì „ë‹¬í•´ì•¼ í•˜ë©°, ë³´í†µ ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ì…ë‹ˆë‹¤.\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant who helps with Python programming.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Can you explain the difference between LLM and chat models in LangChain?\"}\n",
    "  ]\n",
    "  ```\n",
    "- **ì¶œë ¥ í˜•ì‹**:\n",
    "  ```python\n",
    "  {\"role\": \"assistant\", \"content\": \"Sure! LLM and Chat Models differ in their input and interaction styles...\"}\n",
    "  ```\n",
    "- **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€**:\n",
    "  - ë‹¤ì¤‘ í„´ ëŒ€í™”\n",
    "  - ë¬¸ë§¥ ì¶”ì  ë° ìœ ì§€ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜ì˜)\n",
    "  - ëŒ€í™” ê¸°ë°˜ ì±—ë´‡, FAQ ì‹œìŠ¤í…œ ë“±\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ì£¼ìš” ì°¨ì´ì  ìš”ì•½\n",
    "| **íŠ¹ì§•**        | **LLM**                                                | **Chat Model**                                        |\n",
    "|-----------------|------------------------------------------------------|----------------------------------------------------|\n",
    "| **ì…ë ¥ í˜•ì‹**   | ë‹¨ì¼ í…ìŠ¤íŠ¸ ì…ë ¥                                         | ì—­í•  ê¸°ë°˜ì˜ ëŒ€í™” ë©”ì‹œì§€ ê°ì²´ (role: system, user, assistant) |\n",
    "| **ëŒ€í™” íˆìŠ¤í† ë¦¬**| ë¬¸ë§¥ ì¶”ì  ë¶ˆê°€ëŠ¥ (ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬)                          | ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í†µí•´ ë¬¸ë§¥ì„ ìœ ì§€í•˜ê³  ë°˜ì˜                 |\n",
    "| **ì‚¬ìš© ëª©ì **   | í…ìŠ¤íŠ¸ ìƒì„±, ìš”ì•½, ë‹¨ìˆœ ì§ˆì˜ì‘ë‹µ                             | ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤, ì±—ë´‡, ë‹¤ì¤‘ í„´ ì§ˆì˜ì‘ë‹µ               |\n",
    "| **ì‘ìš© ì‚¬ë¡€**   | ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€, í…ìŠ¤íŠ¸ ìƒì„±                                | ê³ ê° ì§€ì› ì±—ë´‡, ì¸í„°ë™í‹°ë¸Œ Q&A, ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 5. ì–¸ì œ ì–´ë–¤ ê²ƒì„ ì„ íƒí•´ì•¼ í• ê¹Œìš”?\n",
    "- **ë‹¨ì¼ ì‘ì—…ì´ë‚˜ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±**:\n",
    "  - `LLM`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì í•©í•©ë‹ˆë‹¤.\n",
    "- **ëŒ€í™” ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë‚˜ ë¬¸ë§¥ì„ ìœ ì§€í•´ì•¼ í•˜ëŠ” ì‘ì—…**:\n",
    "  - `Chat Model`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db362b9d-d4c9-4490-a18a-4774ad11a025",
   "metadata": {},
   "source": [
    "# LangChain ì²« ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cecf24e-7f5d-4a06-b54a-c54025ffefc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.23'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72728b4-bd4d-4887-a6ca-79db0ffd482e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e14c440-95bf-426a-a0c3-15db152a7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_community ëª¨ë“ˆ\n",
    "#  - LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í™•ì¥ ëª¨ë“ˆ\n",
    "#  - LangChainì˜ ê³µì‹ ì»¤ë®¤ë‹ˆí‹°ê°€ ê°œë°œí•œ ì—¬ëŸ¬ í™•ì¥ ê¸°ëŠ¥ì„ í¬í•¨í•œ ëª¨ë“ˆ\n",
    "#  - ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œë‚˜ ìœ í‹¸ë¦¬í‹°ê°€ í¬í•¨\n",
    "#  - ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ìë“¤ì´ LangChainì„ ë” ì˜ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ë„êµ¬ë“¤ì´ í¬í•¨\n",
    "#  - Colab ì—ëŠ” ê¸°ë³¸ ì„¤ì¹˜ ë˜ì–´ ìˆì§€ ì•ŠìŒ (2024.12 í˜„ì¬)\n",
    "\n",
    "#  ê³µì‹ API ë ˆí¼ëŸ°ìŠ¤ https://python.langchain.com/api_reference/community/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9362f0cc-7fef-4557-aef5-cb4547f7438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain-openai ëª¨ë“ˆ\n",
    "#  - LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ OpenAIì˜ ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í™•ì¥ ëª¨ë“ˆ\n",
    "#  - OpenAI APIì™€ì˜ í†µí•©ì„ ê°„ì†Œí™”:  ì‚¬ìš©ìê°€ OpenAIì˜ ëª¨ë¸ì„ ì‰½ê²Œ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í…ìŠ¤íŠ¸ ìƒì„±, ìš”ì•½, ë²ˆì—­, ì§ˆì˜ì‘ë‹µ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "#  - Chain ë° Tool Integration: ë‹¤ì–‘í•œ \"Chain\" ë° \"Tool\"ë“¤ì„ ê²°í•©í•˜ì—¬ ë³µì¡í•œ NLP ì‘ì—…ì„ ìë™í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. langchain-openai ëª¨ë“ˆì€ OpenAI ëª¨ë¸ì„ LangChainì˜ ë‹¤ë¥¸ êµ¬ì„± ìš”ì†Œì™€ ì‰½ê²Œ ì—°ê²°í•  ìˆ˜ ìˆë„ë¡ í•´, ë‹¤ì–‘í•œ ì–¸ì–´ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "#  - â˜…ì‚¬ìš©í•˜ë ¤ë©´ í™˜ê²½ë³€ìˆ˜ì— ë°˜ë“œì‹œ OPENAI_API_KEY ê°’ì´ ìˆì–´ì•¼ í•œë‹¤\n",
    "\n",
    "#  ê³µì‹ API ë ˆí¼ëŸ°ìŠ¤ https://python.langchain.com/api_reference/openai/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d30071b-f39e-486d-9577-294a8e9ab2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms.base import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf76093c-1b0e-4b92-8b79-2b8650621e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models.base import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a181fe-4463-4a05-853c-4d247131ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm ìƒì„±\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfd299f-86a0-4238-a93c-63b843a1601f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-instruct'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "773b9484-65ad-48d7-bab1-eb0a22d610eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat Model ìƒì„±\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chat.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a7b5f-2123-4165-a602-afe9e74ccef0",
   "metadata": {},
   "source": [
    "## LLM í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52fa3b7-a76d-4c49-a94c-6804cd8fdd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ë‹µë³€ \n",
      "\n",
      "As of 2021, there are eight planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Pluto was previously considered a planet but is now classified as a dwarf planet.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"How many planets are there?\")\n",
    "\n",
    "print(type(result))\n",
    "print('ë‹µë³€', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52aa5df-05b2-4345-9c30-456659ad3859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ë‹µë³€ \n",
      "\n",
      "íƒœì–‘ê³„ì—ëŠ” 8ê°œì˜ í–‰ì„±, 5ê°œì˜ ì™œí–‰ì„±, 1ê°œì˜ íƒœì–‘, ê·¸ë¦¬ê³  ê·¸ ì™¸ì—ë„ ì†Œí–‰ì„±, í˜œì„±, ìš´ì„ ë“± ë§ì€ ìƒì„±ë“¤ì´ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"íƒœì–‘ê³„ì—ëŠ” ì–¼ë§ˆë‚˜ ë§ì€ ìƒì„±ë“¤ì´ ìˆë‚˜ìš”?\")\n",
    "\n",
    "print(type(result))\n",
    "print('ë‹µë³€', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88f844-cdcc-4374-a3b8-eb57aa8f1aa8",
   "metadata": {},
   "source": [
    "## ChatModel í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d450ea-bc42-4cf0-a67a-a6cb224b0d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ğŸ’š content='There are currently eight recognized planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 13, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0The4OP4tJgMGPweHDAmzbqubD4', 'finish_reason': 'stop', 'logprobs': None} id='run--dc38d610-5987-46c8-80b0-0f281d5f4f7b-0' usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "ğŸ§¡ë‹µë³€ There are currently eight recognized planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"How many planets are there?\")  # ì…ë ¥ìœ¼ë¡œ str ê°€ëŠ¥.\n",
    "\n",
    "print(type(result))  # Message ê°ì²´\n",
    "print('ğŸ’š', result)\n",
    "print('ğŸ§¡ë‹µë³€', result.content) # â˜…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeeaa46-a11c-4f03-8c31-a778d02b4b96",
   "metadata": {},
   "source": [
    "## í•œê¸€ or ì˜ì–´ ?\n",
    "\n",
    "ì±— GPTì˜ ì–¸ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì€ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ í›Œë¥­í•œ ë°œì „ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‚¬ìš©ìê°€ ë°›ëŠ” ë‹µë³€ì˜ í’ˆì§ˆì€ ì œì¶œí•˜ëŠ” ì–¸ì–´ì— ë”°ë¼ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ì°¨ì´ëŠ” ì±— GPTê°€ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ë°ì´í„° ì–‘ê³¼ í’ˆì§ˆ, ê·¸ë¦¬ê³  ì–¸ì–´ë³„ íŠ¹ì„±ì„ ì–¼ë§ˆë‚˜ ì˜ ì²˜ë¦¬í•˜ëŠ”ì§€ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "\n",
    "OpenAIì˜ ì–¸ì–´ ëª¨ë¸, íŠ¹íˆ GPT ì‹œë¦¬ì¦ˆëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì–»ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ í•™ìŠµë©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ì£¼ë¡œ ì˜ì–´ë¥¼ ë¹„ë¡¯í•œ ì—¬ëŸ¬ ì–¸ì–´ì—ì„œ ìˆ˜ì§‘ë˜ë©°, í•™ìŠµ ë°ì´í„°ì˜ êµ¬ì„±ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ì–´ëŠ” ì „ì„¸ê³„ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ë©°*, ì¸í„°ë„· ìƒì˜ ë°ì´í„°ë„ ì˜ì–´ê°€ ë§ì•„ì„œ ì±— GPTëŠ” ì˜ì–´ ì§ˆë¬¸ì— ëŒ€í•´ ë” ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•  í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í•œêµ­ì–´ì™€ ê°™ì€ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜, ì–¸ì–´ì˜ ë³µì¡ì„± ë•Œë¬¸ì— ì²˜ë¦¬ê°€ ë” ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´, ì´ë¡œ ì¸í•´ ë‹µë³€ì˜ í’ˆì§ˆì— ì°¨ì´ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³ \n",
    "- https://fastcampus.co.kr/gov_review_insightGPTlang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07501ad9-ad10-4301-9bcd-53a449b276e4",
   "metadata": {},
   "source": [
    "# Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcd552f5-d30d-43ea-a4f7-69a4454f3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModel ì€ 'ì§ˆë¬¸'ë§Œ ë°›ëŠ”ê²Œ ì•„ë‹ˆë¼ 'ëŒ€í™”' ë„ í• ìˆ˜ ìˆë‹¤ (Message ë¥¼ ë³´ë‚¼ìˆ˜ë„ ìˆë‹¤)\n",
    "# 'ëŒ€í™”(conversation)' ì€\n",
    "#    : ì—¬ëŸ¬ ë©”ì„¸ì§€ ë¬¶ìŒ\n",
    "#    : ìƒëŒ€ì˜ ëŒ€í™”ì˜ ë§¥ë½ì— ë§ê²Œ ëŒ€ë‹µí• ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcfe8e0e-11ce-4442-87d1-e8a629827efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/chat/openai/#instantiation\n",
    "# ë ˆí¼ëŸ°ìŠ¤ : https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n",
    "\"\"\"\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "        # ëª¨ë¸ì˜ ì‘ë‹µ ë‹¤ì–‘ì„±ì„ ì œì–´í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "        # ì´ëŠ” OpenAIì˜ GPT ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ,\n",
    "        #  ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ì°½ì˜ì„±ê³¼ í™•ë¥ ì  ë‹¤ì–‘ì„±(ëœë¤ì„±ì„ ì¡°ì •í•©ë‹ˆë‹¤)ã„´\n",
    "        #\n",
    "    max_tokens=None,  # modelì´ ë¦¬í„´í•˜ëŠ” ê²°ê³¼ì˜ ìµœëŒ€ token ê°œìˆ˜ì§€ì •.\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33554dd7-195a-4125-99c9-901f4b4a3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0421f8-5f08-4b55-8c1d-ce0dd501bf79",
   "metadata": {},
   "source": [
    "## Human / System / AI Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e9f0ef6-dc2b-47ed-83d2-bc3ef2fc3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html\n",
    "\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html\n",
    "\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html\n",
    "\n",
    "# HumanMessage : ì‚¬ëŒì´ AI ì— ë³´ë‚´ëŠ” Message\n",
    "# SystemMessage : LLM ì— ì„¤ì •ë“¤ì„ ì œê³µí•˜ê¸° ìœ„í•œ Message\n",
    "# AIMessage: AI ì— ì˜í•´ ë¦¬í„´ë˜ëŠ” Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec8158b-af87-47b1-9e98-939a26d8a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content = \"You are a geography expert. And your only reply in Korean\",\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content = \"ì•ˆë…•, ë‚´ ì´ë¦„ì€ ë‘˜ë¦¬ ì•¼\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content = \"\"\"What is the distance between Mexico and Thailand.\n",
    "          Also, what is your name?\"\"\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f35dcf4-9ab3-4943-b7fd-e4b37b0aa5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¡ <class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ë©•ì‹œì½”ì™€ íƒœêµ­ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì•½ 16,000kmì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ ë‘˜ë¦¬ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 58, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0TiEzlYFpUIMGm2HBNmVdMzADlu', 'finish_reason': 'stop', 'logprobs': None}, id='run--c5cf4633-8949-49ac-9910-b892c105e8ea-0', usage_metadata={'input_tokens': 58, 'output_tokens': 35, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat.invoke(messages)\n",
    "print('ğŸ§¡', type(result))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "349294e4-a92f-4ddc-b06c-34603ec246df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë©•ì‹œì½”ì™€ íƒœêµ­ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì•½ 16,000kmì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ ë‘˜ë¦¬ì…ë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77789ce6-878d-4968-993f-6ed4dff3ea14",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2377c33-34e8-4094-8389-af88884b495d",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "â†‘ messages ë¥¼ prompt ë¼ê³ ë„ í•¨ (?)\n",
    "- ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°\n",
    "- ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ê±°ë‚˜, ëª¨ë¸ì´ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µ\n",
    "- LLM ê³¼ ì˜ì‚¬ì†Œí†µí•˜ê¸° ìœ„í•œ ë°©ë²•\n",
    "\n",
    "---\n",
    "\n",
    "LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì—ì„œ **í”„ë¡¬í”„íŠ¸ prompt**ë€ ëª¨ë¸ì— **ì…ë ¥**ìœ¼ë¡œ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ê±°ë‚˜, ëª¨ë¸ì´ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì˜ ì¶œë ¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ì˜ ì—­í• :\n",
    "1. **ëª¨ë¸ì— ëŒ€í•œ ì§€ì‹œ**: í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì—ê²Œ ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìê°€ ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜, íŠ¹ì • ìŠ¤íƒ€ì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•  ë•Œ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. **ì»¨í…ìŠ¤íŠ¸ ì œê³µ**: ëª¨ë¸ì´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ë°°ê²½ ì •ë³´ë‚˜ ë¬¸ë§¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ì£¼ì œì— ëŒ€í•œ ì§ˆë¬¸ì„ í•  ë•Œ, ê´€ë ¨ ë°°ê²½ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ì´ ë” ì •í™•í•œ ë‹µì„ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ëª¨ë¸ì˜ ì¶œë ¥ ìœ ë„**: í”„ë¡¬í”„íŠ¸ê°€ ëª¨ë¸ì˜ ì¶œë ¥ì„ ìœ ë„í•˜ê³ , ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ìŠ¤íƒ€ì¼, ë‚´ìš©, í˜•ì‹ ë“±ì„ ê²°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "### ì˜ˆì‹œ:\n",
    "1. **ì§ˆë¬¸ ì‘ë‹µ**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"What is the capital of France?\"\n",
    "   - **ì¶œë ¥**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **ì°½ì˜ì  ê¸€ì“°ê¸°**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **ì¶œë ¥**: ëª¨ë¸ì´ ì°½ì˜ì ìœ¼ë¡œ ë“œë˜ê³¤ê³¼ ê¸°ì‚¬ì— ê´€í•œ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ë²ˆì—­**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **ì¶œë ¥**: \"Hola, Â¿cÃ³mo estÃ¡s?\"\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ì˜ ì¢…ë¥˜:\n",
    "- **ë‹¨ìˆœí•œ ì§ˆë¬¸**: ì‚¬ìš©ìê°€ ë‹¨ìˆœíˆ ê¶ê¸ˆí•œ ì ì„ ë¬»ëŠ” í˜•íƒœ.\n",
    "- **ì§€ì‹œë¬¸**: íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ëŠ” í˜•íƒœ.\n",
    "- **í˜•ì‹í™”ëœ ì…ë ¥**: íŠ¹ì • í˜•ì‹ì´ë‚˜ êµ¬ì¡°ë¥¼ ê°–ì¶˜ ì…ë ¥(ì˜ˆ: í…ìŠ¤íŠ¸ ìš”ì•½, ë²ˆì—­, ì½”ë“œ ì‘ì„± ë“±).\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì„¤ê³„ì˜ ì¤‘ìš”ì„±:\n",
    "- **ì •í™•í•œ ê²°ê³¼**ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” **í”„ë¡¬í”„íŠ¸ì˜ ì„¤ê³„**ê°€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•˜ë©´ ëª¨ë¸ì´ ì›í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n",
    "- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤í—˜í•˜ë©´ì„œ ëª¨ë¸ì˜ ë°˜ì‘ì„ ê´€ì°°í•˜ê³ , ê°€ì¥ ì í•©í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì„¤ê³„ íŒ:\n",
    "1. **ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§€ì‹œ**: ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, \"Explain quantum mechanics\"ë³´ë‹¤ëŠ” \"Explain quantum mechanics in simple terms for a high school student\"ì™€ ê°™ì´ êµ¬ì²´ì ì¸ ìš”êµ¬ë¥¼ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "   \n",
    "2. **ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ**: í•„ìš”í•œ ë°°ê²½ ì •ë³´ë‚˜ ë¬¸ë§¥ì„ ì œê³µí•˜ë©´ ëª¨ë¸ì´ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "3. **ë‹¤ì–‘í•œ ì‹¤í—˜**: í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ê¸ˆì”© ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸í•´ ë³´ë©´ì„œ ìµœì ì˜ ì‘ë‹µì„ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ê²°ë¡ :\n",
    "í”„ë¡¬í”„íŠ¸ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ì§€ì‹œí•˜ëŠ” ì¤‘ìš”í•œ ì…ë ¥ìœ¼ë¡œ, ëª¨ë¸ì´ ìˆ˜í–‰í•  ì‘ì—…ì˜ ë°©í–¥ì„ ê²°ì •ì§“ëŠ” ìš”ì†Œì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ ì„¤ê³„í•˜ëŠ” ê²ƒì´ LLMì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë° í° ë„ì›€ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b8190-c4f8-41b6-b283-e461ae6e8800",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "ë©”ì„¸ì§€ ì»¤ìŠ¤í„°ë§ˆì´ì§•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a9b37e0-60ef-46d8-a680-3dcdabe9db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts.chat import ChatMessagePromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce0ac97b-ecdc-4ce4-b7b9-84413edbdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate ëŠ” message(s) ë¡œë¶€í„° template ì„ ë§Œë“¬.\n",
    "# PromptTemplate ëŠ” string ì„ ì´ìš©í•´ì„œ template ì„ ë§Œë“¬.\n",
    "#  â†‘ ë‘˜ë‹¤ ìœ ìš©í•˜ê²Œ ì“°ì„."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba7dfaa8-6363-4973-b858-52728626bd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='What is the distance between {country_a} and {country_b}')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    \"What is the distance between {country_a} and {country_b}\"\n",
    ")\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67f1ada9-e019-493c-811f-7a6f51fffe1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the distance between Mexico and Thailand'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template.format()  # KeyError\n",
    "prompt = template.format(country_a = \"Mexico\", country_b = \"Thailand\")\n",
    "\n",
    "prompt # str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c51cad9b-1da3-4a0e-8474-b22ce8977879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The distance between Mexico and Thailand is approximately 9,500 miles (15,300 kilometers) when measured in a straight line.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0TjhBq1tvqsGIJLX26ADHPo5UDo', 'finish_reason': 'stop', 'logprobs': None}, id='run--b5e5bdfe-4e98-42cc-a0c9-715dc1494a36-0', usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc39b5-4cf1-4b63-b88d-030ba39ebeb0",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91f2ea1b-b842-4998-ae58-73cc04e780d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['country_a', 'country_b', 'language', 'name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a geography expert. And your only reply in {language}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='ì•ˆë…•, ë‚´ ì´ë¦„ì€ {name} ì•¼'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='\\n        What is the distance between {country_a} and {country_b}.\\n        Also, what is your name?\\n        '), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    # SystemMessage íŠœí”Œ\n",
    "    (\"system\", \"You are a geography expert. And your only reply in {language}\"),\n",
    "    # AIMessage íŠœí”Œ\n",
    "    (\"ai\", \"ì•ˆë…•, ë‚´ ì´ë¦„ì€ {name} ì•¼\"),\n",
    "    # HumanMessage íŠœí”Œ\n",
    "    (\"human\", \"\"\"\n",
    "        What is the distance between {country_a} and {country_b}.\n",
    "        Also, what is your name?\n",
    "        \"\"\"),\n",
    "])\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e38f5a69-4eb8-45ee-bb23-2b3ab35fafa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a geography expert. And your only reply in Korean', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•ˆë…•, ë‚´ ì´ë¦„ì€ ë½€ë¡œë¡œ ì•¼', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n        What is the distance between Canada and Japan.\\n        Also, what is your name?\\n        ', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format_messages(\n",
    "    language=\"Korean\",\n",
    "    name=\"ë½€ë¡œë¡œ\",\n",
    "    country_a=\"Canada\",\n",
    "    country_b=\"Japan\",    \n",
    ")\n",
    "\n",
    "print(type(prompt))  # List[Message]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bae6edd5-1586-4490-832a-45b8fa95b0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ìºë‚˜ë‹¤ì™€ ì¼ë³¸ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì•½ 7,000kmì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ ë½€ë¡œë¡œì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 62, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0Tlw1gfMGHhQuSfL7ICHTN678gX', 'finish_reason': 'stop', 'logprobs': None}, id='run--66b8ff7b-4721-4de7-8c9a-3a0a072d32ea-0', usage_metadata={'input_tokens': 62, 'output_tokens': 35, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378f87d-b695-4b4c-b2f6-04f0ce37ea18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "468282bf-ae1d-410f-b808-b0eadeb244b5",
   "metadata": {},
   "source": [
    "# OutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40039b72-ce91-40b6-9787-8c164e3ae474",
   "metadata": {},
   "source": [
    "## Output Parser ë€\n",
    "\n",
    "LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì—ì„œ ìƒì„±ëœ ì¶œë ¥ì„ ì²˜ë¦¬í•˜ê³  'ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜'í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ìœ í‹¸ë¦¬í‹°ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ 'êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜'í•˜ê±°ë‚˜, 'íŠ¹ì • ê·œì¹™ì— ë”°ë¼ ë°ì´í„°ë¥¼ ì¶”ì¶œ'í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "\n",
    "1. ì¶œë ¥ êµ¬ì¡°í™”\n",
    "    - ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ JSON, ë”•ì…”ë„ˆë¦¬, ëª©ë¡ ë“±ê³¼ ê°™ì€ í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    \n",
    "1. ì¶œë ¥ ê²€ì¦\n",
    "    - ëª¨ë¸ì´ ì˜ˆìƒì¹˜ ëª»í•œ ì¶œë ¥ì„ ë°˜í™˜í•  ê²½ìš° ì ì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì œê³µí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•˜ë„ë¡ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    \n",
    "1. ì¶œë ¥ í‘œì¤€í™”\n",
    "    - ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì´ í•­ìƒ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c023e-8581-41ba-a6cc-e35f707f83a1",
   "metadata": {},
   "source": [
    "## BaseOuputParser\n",
    "ì»¤ìŠ¤í…€ OutputParser ë¥¼ ì •ì˜í•´ ì‚¬ìš©í•´ë³´ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cd0b8ca-aa92-4e03-b56a-916f706a6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.base import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f766af0-959e-4df4-a449-7eeacbb23fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ì˜ ì¶œë ¥ì„ list ë¡œ ë³€í™˜í•˜ëŠ” OutputParser ë¥¼ ë§Œë“¤ì–´ ë³´ì\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    # parse() ë©”ì†Œë“œë¥¼ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•œë‹¤\n",
    "    #   text=  ì…ë ¥í…ìŠ¤íŠ¸\n",
    "    def parse(self, text):  # text <- \"aaa,bbb,ccc,ddd\"\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc7df14f-fb83-4f74-b0d7-e9b7274649c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = CommaOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fad86122-0b1d-43a9-9e5a-e48e20764283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.parse(\"   Hello,   how  ,  are  ,you   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6d7792a-daf1-4ce6-aaa4-72d7fb50b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 51, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0TlIpdL73oarbeOrGFYFFRGfQHH', 'finish_reason': 'stop', 'logprobs': None}, id='run--38d5a9ef-1ba9-4680-be2b-7debec6dcc6f-0', usage_metadata={'input_tokens': 51, 'output_tokens': 32, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "        You are a list generating machine.\n",
    "        Everything you are asked will be answered with a list of max {max_items}.\n",
    "        Do NOT reply with anything else.    \n",
    "    \"\"\"),\n",
    "    ('human', \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items = 10,\n",
    "    question = \"What are the planets?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "print(result.__repr__())\n",
    "print('ğŸ¹' * 30)\n",
    "print(result.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57705e99-fb14-4a0a-901c-088eb38405b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 53, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0Tn7tI3GssIMS1p2arYHDwo9JWR', 'finish_reason': 'stop', 'logprobs': None}, id='run--d5d88c63-4254-4fe3-96f3-c0510fbc77ec-0', usage_metadata={'input_tokens': 53, 'output_tokens': 17, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹\n",
      "Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "        You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.    \n",
    "    \"\"\"),\n",
    "    ('human', \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items = 10,\n",
    "    question = \"What are the planets?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "print(result.__repr__())\n",
    "print('ğŸ¹' * 30)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7ba3c28-9b35-4927-a085-d82866d75d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='red, blue, green, yellow, orange, purple, pink, black, white, brown', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 53, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0Tobuz0V9OGZqePCVBFmoKSS2wX', 'finish_reason': 'stop', 'logprobs': None}, id='run--9607f809-07ae-4a18-9128-de5d95dc2ed6-0', usage_metadata={'input_tokens': 53, 'output_tokens': 19, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹\n",
      "red, blue, green, yellow, orange, purple, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "        You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.    \n",
    "    \"\"\"),\n",
    "    ('human', \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items = 10,\n",
    "    question = \"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "print(result.__repr__())\n",
    "print('ğŸ¹' * 30)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "548a5715-81c1-4685-a0b2-0b7be33db557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='red, blue, green, yellow, orange, purple, pink, black, white, brown', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 55, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0TpCinZBOoReJeN8y70fP8XvlQ1', 'finish_reason': 'stop', 'logprobs': None}, id='run--5fbfa55b-660f-4a38-a2fe-64c414c60269-0', usage_metadata={'input_tokens': 55, 'output_tokens': 19, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹ğŸ¹\n",
      "red, blue, green, yellow, orange, purple, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "        You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.    \n",
    "    \"\"\"),\n",
    "    ('human', \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items = 10,\n",
    "    question = \"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "print(result.__repr__())\n",
    "print('ğŸ¹' * 30)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c9662bb-e136-4dda-9878-beb14ac2e238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "        You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.    \n",
    "    \"\"\"),\n",
    "    ('human', \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items = 10,\n",
    "    question = \"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d85e78-1cdf-49f0-95d6-37bd8792d5c8",
   "metadata": {},
   "source": [
    "# â–  Chain, LCEL\n",
    "\n",
    "- LCEL (LangChain Expression Language: ë­ì²´ì¸ í‘œí˜„ ì–¸ì–´)\n",
    "  - LCELì€ LangChain ë‚´ì—ì„œ ë³µì¡í•œ í‘œí˜„ì‹ì„ ì²˜ë¦¬í•˜ê³ ,\n",
    "  - ëª¨ë¸ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ë” ê°•ë ¥í•˜ê³  ìœ ì—°í•˜ê²Œ ë§Œë“œëŠ” ê¸°ëŠ¥ì„ ì œê³µ\n",
    "    - ì½”ë“œì–‘ì„ ë§ì´ ì¤„ì—¬ì¤Œ.\n",
    "    - ë‹¤ì–‘í•œ template ê³¼ LLM í˜¸ì¶œ\n",
    "    - ì„œë¡œ ë‹¤ë¥¸ ì‘ë‹µ(response) ë¥¼ í•¨ê»˜ ì‚¬ìš©ì¼€ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c229aa6-4612-4bb6-8fd3-74431aeffac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['max_items', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['max_items'], input_types={}, partial_variables={}, template='\\n        You are a list generating machine.\\n        Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.\\n        Do NOT reply with anything else.    \\n    '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000021F5B8F7500>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021F5BC95BB0>, root_client=<openai.OpenAI object at 0x0000021F5B8F7B30>, root_async_client=<openai.AsyncOpenAI object at 0x0000021F5B933140>, temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| CommaOutputParser()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain ì´ë¼ëŠ” ê²ƒì€ ê¸°ë³¸ì ìœ¼ë¡œ, ëª¨ë“  ìš”ì†Œë¥¼ ì—°ê²°í•´ì£¼ëŠ”(í•©ì³ì£¼ëŠ”) ì—­í• ì„ í•¨.\n",
    "# í•©ì³ì§„ ìš”ì†Œë“¤ì€ í•˜ë‚˜ì˜ chain ìœ¼ë¡œ ì‹¤í–‰ë ê²ë‹ˆë‹¤.\n",
    "\n",
    "# chain ìƒì„±!\n",
    "#  '|' ì—°ì‚°ì ì‚¬ìš©.\n",
    "#  LangChain ì˜ í•µì‹¬!\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "print(type(chain))\n",
    "\n",
    "chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e57323bd-f06c-4821-aaf8-0172f89dbbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charmander', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain í˜¸ì¶œ.  invoke()\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the pokemons?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a24d790a-14c2-4ac8-943c-c44d7b598f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "â†‘ Chain ì„ ì‚¬ìš©í•´ ê½¤ë‚˜ ê°„ê²°í•œ ì½”ë“œë¡œ ì‘ë™ëœë‹¤!\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({...})\n",
    "ì‚¬ì‹¤ ë­ì²´ì¸ì€ ë‚´ë¶€ì—ì„œ\n",
    "  .format_message() í˜¸ì¶œ -> prompt ì™„ì„±\n",
    "  -> chat.invoke() í˜¸ì¶œ -> AIMessage ë¦¬í„´\n",
    "  -> parse() í˜¸ì¶œí•œë‹¤\n",
    "\n",
    "ì´ëŸ¬í•œ ì¼ë ¨ì˜ ì‘ì—…ì„ chain.invoke() í˜¸ì¶œ ë‹¨í•œë²ˆìœ¼ë¡œ ëë‚¸ë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ chain êµ¬ë¬¸ìœ¼ë¡œ ì •ë§ ë‹¤ì–‘í•œ ì‘ì—…ì˜ íë¦„ë“¤ì„ ìˆ˜í–‰í• ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a986bc0-3264-443b-8cb6-002e9f91e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chain ë¼ë¦¬ë„ ê²°í•©í• ìˆ˜ ë„ ìˆë‹¤.\n",
    "\n",
    "[ì˜ˆì‹œ]\n",
    "chain_one = template | chat | CommaOutputParser()\n",
    "chain_two = template_2 | chat | OutputParser2()\n",
    "\n",
    "all = chain_one | chain_two | OutputParser3()\n",
    "  â†‘ chain_one ì˜ ì¶œë ¥ì„ chain_two ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe9dd6-a74e-4b39-8c79-36bbf8eb2ac4",
   "metadata": {},
   "source": [
    "# Chaining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4886bbdf-e48a-41b3-a17f-20ec5f30fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²«ë²ˆì§¸ chain\n",
    "#   íŠ¹ì • ìš”ë¦¬ì— ëŒ€í•œ recipe ë¥¼ ì œê³µí•´ì£¼ëŠ” chef chain\n",
    "\n",
    "# ë‘ë²ˆì§¸ chain\n",
    "#    ìš”ë¦¬ì˜ recipeë¥¼ ë°›ì•„ì„œ ì±„ì‹ì£¼ì˜ììš© recipe ë¡œ ë³€í™˜í•´ì£¼ëŠ” veg_chef chain\n",
    "\n",
    "# ìµœì¢… chain \n",
    "#  ìœ„ ë‘ê°œì˜ chain ì„ ì—°ê²°í•œ chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eca01831-2f1b-4b53-b72b-f4de9ecdf09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²«ë²ˆì§¸ chain\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"\n",
    "      You are a world-class international chef.\n",
    "      You create easy to follow recipes for any type of cuisines\n",
    "      with easy to find ingredients.    \n",
    "    \"\"\"),\n",
    "    ('human', \"\"\"\n",
    "    I want to cook {cuisine} food.\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9394a94f-613a-4379-b3f4-be5fefea3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ë²ˆì§¸ chain\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"\n",
    "      You are a vegetarian chef specialized on\n",
    "      making traditional recipies vegetarian.\n",
    "      You find alternative ingredients and explain their preparation.\n",
    "      You don't radically modify the recipe.\n",
    "      If there is no alternative for a food just say\n",
    "      you don't know how to replace it.    \n",
    "    \"\"\"),\n",
    "    ('human', \"\"\"\n",
    "        {recipe}\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afeb6a22-eb30-4002-944d-89775927a914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\\n\\nIngredients:\\n- 1 lb firm tofu or paneer, cut into bite-sized pieces\\n- 1 cup plain yogurt (use plant-based yogurt for a vegan version)\\n- 2 tablespoons lemon juice\\n- 2 tablespoons vegetable oil\\n- 2 tablespoons garam masala\\n- 1 tablespoon ground cumin\\n- 1 tablespoon paprika\\n- 1 tablespoon minced garlic\\n- 1 tablespoon minced ginger\\n- 1 teaspoon turmeric\\n- 1 teaspoon ground coriander\\n- 1 teaspoon chili powder (adjust to taste)\\n- Salt and pepper to taste\\n- 1 can (14 oz) tomato sauce\\n- 1 cup coconut cream (or any plant-based heavy cream)\\n- Fresh cilantro for garnish\\n\\nInstructions:\\n1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\\n\\n2. Instead of baking, you can pan-fry the marinated tofu or paneer until they are golden brown and cooked through.\\n\\n3. In a large skillet, heat some oil over medium heat. Add the tomato sauce and simmer for 5 minutes. Stir in the coconut cream (or plant-based heavy cream) and continue to simmer for another 5 minutes.\\n\\n4. Add the cooked tofu or paneer pieces to the sauce and simmer for an additional 10 minutes to allow the flavors to blend.\\n\\n5. Serve the Vegetarian Tikka Masala over steamed rice or with naan bread. Garnish with fresh cilantro before serving.\\n\\nEnjoy your vegetarian version of Tikka Masala! Let me know if you have any questions or if you'd like more vegetarian Indian recipes.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 393, 'prompt_tokens': 776, 'total_tokens': 1169, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj0inGOtU9GNrciY6PtQ6sDAwp7UM', 'finish_reason': 'stop', 'logprobs': None}, id='run--e344913b-9e68-40c7-b17b-a4f597e57089-0', usage_metadata={'input_tokens': 776, 'output_tokens': 393, 'total_tokens': 1169, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final chain\n",
    "\n",
    "# final_chain = chef_chain | veg_chain\n",
    "\n",
    "# chef_chain ì˜ output ì´ veg_chain ì˜ {recipe} ì…ë ¥ê°’ìœ¼ë¡œ ì „ë‹¬ë˜ê²Œ í•˜ê¸°\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "result = final_chain.invoke({\n",
    "    \"cuisine\": \"indian\"  # ì²«ë²ˆì§¸ chain ì¸ chef_chain ì˜ chef_prompt ì˜ {cuisine} ì— ì „ë‹¬\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5189e7e6-8e05-4559-b6f0-2801645fb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 tablespoons garam masala\n",
      "- 1 tablespoon ground cumin\n",
      "- 1 tablespoon ground coriander\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 can (14 oz) crushed tomatoes\n",
      "- 1 cup coconut cream (or dairy-free heavy cream)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (to serve)\n",
      "\n",
      "**Instructions:**\n",
      "1. Follow the same marinating process as the original recipe but use tofu or paneer instead of chicken. Marinate for at least 1 hour.\n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until golden brown on all sides.\n",
      "3. Continue with the recipe as instructed, replacing chicken with the cooked tofu or paneer when adding it to the skillet.\n",
      "4. Use coconut cream or dairy-free heavy cream to maintain the creamy texture of the dish.\n",
      "5. Adjust the seasoning as needed and serve the Vegetarian Tikka Masala over rice or with naan bread, garnished with chopped cilantro.\n",
      "\n",
      "Enjoy your flavorful Vegetarian Tikka Masala! Let me know if you have any other traditional recipes you'd like to make vegetarian.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85655210-b80e-4b4a-ac1d-76554e1673d4",
   "metadata": {},
   "source": [
    "## streaming= ê³¼ callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5967b3-41c2-4324-a37c-566f675e3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â†‘ ì „ë¶€ ì‹¤í–‰ ì™„ë£Œ ë ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ”ê²Œ ì§€ë£¨í•˜ë‹¤.\n",
    "#   ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ë„ ê¶ê¸ˆí•˜ë‹¤.\n",
    "#   ì§„í–‰ë˜ëŠ” ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ í• ìˆ˜ ìˆë‹¤!\n",
    "\n",
    "# Chat model ì˜ streaming=\n",
    "#  streaming ì€ LLM model ì˜ ì‘ë‹µ(resposne) ì´ ìƒì„±ë˜ëŠ” ê²ƒì„\n",
    "#    ì‹¤ì‹œê°„ìœ¼ë¡œ(?) ë³´ê²Œ í•´ì¤Œ.\n",
    "\n",
    "# callbacks=[StreamingStdOutCallbackHandler()]\n",
    "#    ë³¼ìˆ˜ ìˆëŠ” ë¬¸ì(í† í°)ê°€ ìƒê¸¸ ë•Œë§ˆë‹¤ print í•´ì¤€ë‹¤.\n",
    "\n",
    "# callbacks ëŠ” ë‹¤ì–‘í•œ 'event' ê°ì§€ë„ ê°€ëŠ¥\n",
    "#    LLM ì´ ì‘ì—…ì„ ì‹œì‘í–ˆë‹¤ê±°ë‚˜, ëëƒˆë‹¤ê±°ë‚˜.\n",
    "#    ë¬¸ìë¥¼ ìƒì„±í–ˆë‹¤ê±°ë‚˜, ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd6d7ded-f2e7-44a8-ba66-2a50b8cd501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2eb8994c-de5e-4cdd-a41c-cabe5eaebae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f832b1a-a8ac-439f-8c3d-eee6f81b0dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! Indian cuisine is full of delicious flavors and spices. Let's make a classic dish - Chicken Tikka Masala. Here's a simple recipe for you to try at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb boneless, skinless chicken breasts, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 tablespoons garam masala\n",
      "- 1 tablespoon ground cumin\n",
      "- 1 tablespoon ground coriander\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 can (14 oz) crushed tomatoes\n",
      "- 1 cup heavy cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (to serve)\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, mix together the yogurt, lemon juice, 1 tablespoon of vegetable oil, garam masala, cumin, coriander, turmeric, paprika, chili powder, salt, and pepper. Add the chicken pieces and coat them well with the marinade. Cover and refrigerate for at least 1 hour, or overnight for best results.\n",
      "\n",
      "2. Preheat the oven to 400Â°F (200Â°C). Thread the marinated chicken onto skewers and place them on a baking sheet. Bake for 20-25 minutes or until the chicken is cooked through.\n",
      "\n",
      "3. In a large skillet, heat the remaining tablespoon of vegetable oil over medium heat. Add the chopped onion and cook until softened, about 5 minutes. Add the garlic and ginger, and cook for another minute.\n",
      "\n",
      "4. Stir in the crushed tomatoes and simmer for 10 minutes. Add the heavy cream and cooked chicken tikka pieces. Simmer for an additional 10 minutes, stirring occasionally.\n",
      "\n",
      "5. Taste and adjust seasoning as needed. Serve the Chicken Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "Enjoy your homemade Chicken Tikka Masala! Feel free to adjust the spice levels to suit your taste preferences.For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 tablespoons garam masala\n",
      "- 1 tablespoon ground cumin\n",
      "- 1 tablespoon ground coriander\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 can (14 oz) crushed tomatoes\n",
      "- 1 cup coconut cream (as a substitute for heavy cream)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (to serve)\n",
      "\n",
      "**Instructions:**\n",
      "1. Follow the same marinating process as the original recipe but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\n",
      "   \n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until they are golden brown and cooked through.\n",
      "\n",
      "3. In the skillet, proceed with sautÃ©ing the onion, garlic, and ginger as instructed in the original recipe.\n",
      "\n",
      "4. Continue with adding the crushed tomatoes and simmering for 10 minutes. Then, add the coconut cream and the cooked tofu or paneer. Simmer for an additional 10 minutes.\n",
      "\n",
      "5. Adjust the seasoning as needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "By making these simple swaps, you can enjoy a flavorful and satisfying Vegetarian Tikka Masala that stays true to the essence of the traditional dish. Enjoy your meatless version of this classic Indian recipe!âœ… For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 tablespoons garam masala\n",
      "- 1 tablespoon ground cumin\n",
      "- 1 tablespoon ground coriander\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 can (14 oz) crushed tomatoes\n",
      "- 1 cup coconut cream (as a substitute for heavy cream)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (to serve)\n",
      "\n",
      "**Instructions:**\n",
      "1. Follow the same marinating process as the original recipe but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\n",
      "   \n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until they are golden brown and cooked through.\n",
      "\n",
      "3. In the skillet, proceed with sautÃ©ing the onion, garlic, and ginger as instructed in the original recipe.\n",
      "\n",
      "4. Continue with adding the crushed tomatoes and simmering for 10 minutes. Then, add the coconut cream and the cooked tofu or paneer. Simmer for an additional 10 minutes.\n",
      "\n",
      "5. Adjust the seasoning as needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "By making these simple swaps, you can enjoy a flavorful and satisfying Vegetarian Tikka Masala that stays true to the essence of the traditional dish. Enjoy your meatless version of this classic Indian recipe!\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ Chat model ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ê¸°\n",
    "chef_chain = chef_prompt | chat\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "result = final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",  # chef_chain ì˜ {cuisine} ì— ì „ë‹¬\n",
    "})\n",
    "\n",
    "print('\\nâœ…', result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335c44f-558c-462a-a15c-7627592b300f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aadf43-fea8-4ebc-875c-5656c2e82b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf1d81-1dc6-498b-b2f5-d98df9cb5ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18415dca-740b-4b77-bbc6-dbddc1934709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ad36b-f4d3-4134-9ddf-27e8f481341c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e170c9-7fb3-40e4-81ba-32fee1f70742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98019672-67d6-439a-9849-cd34d1505af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b39461-28f1-4d61-8bed-6c978a01ea3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b5309-fc4a-415f-aca1-60881e45ec03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2d73f-983c-4362-a615-699a927e73b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c74489-86c1-4921-a3ef-4ae998138bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cc1bf-f169-4e14-9c9d-2f4352df8166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6a899-6a50-43a6-809a-eaf670706776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b031e-6390-44e4-857d-2b62f2a3d815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d16c93-e04d-40b5-8df6-0a00740c02c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33f3cf-bbe0-41c7-b27e-a8ee22730950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6943a9-5d9c-42f0-a46f-a8c391335d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58fc1c4-6128-4a22-ab22-b68931dd482f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0b672-de0d-4342-be96-7d65d0903a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
