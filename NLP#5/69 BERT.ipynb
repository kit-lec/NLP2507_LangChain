{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNZFPr7BccRCEPRXQxFN/D0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3db2ab4d523043339c1c0e8e52bb0d4f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7bebafb4317408a9b132e58e63ae24b","IPY_MODEL_fb8d82bd26f24c449b6244e95cb567b8","IPY_MODEL_0c31aec0f5054a3fbebc120660a8c99a"],"layout":"IPY_MODEL_0dfc6c51ac824ca7bb2214fa6817305b"}},"f7bebafb4317408a9b132e58e63ae24b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74f4bb6362464787a0355474f36f40fc","placeholder":"​","style":"IPY_MODEL_d3383a67f9224f1f98e5ddc7d55ae0b7","value":"tokenizer_config.json: 100%"}},"fb8d82bd26f24c449b6244e95cb567b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc093a2dab53498198e48267f749ba1f","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffc3f7f549114888915ca29c7b6c9d3d","value":48}},"0c31aec0f5054a3fbebc120660a8c99a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e0fd61b6157476ba94b3f244c99e572","placeholder":"​","style":"IPY_MODEL_63257a0b8d67493fb2b37fdd36282b33","value":" 48.0/48.0 [00:00&lt;00:00, 5.87kB/s]"}},"0dfc6c51ac824ca7bb2214fa6817305b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74f4bb6362464787a0355474f36f40fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3383a67f9224f1f98e5ddc7d55ae0b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc093a2dab53498198e48267f749ba1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffc3f7f549114888915ca29c7b6c9d3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e0fd61b6157476ba94b3f244c99e572":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63257a0b8d67493fb2b37fdd36282b33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbed109dfc1945519647ebaa885abbdd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d64be872bbd9472aa04749f836cceeb2","IPY_MODEL_9fba97b7f28743029128d9e0681fcf9b","IPY_MODEL_e5183b1f75ba4a03b4f4372a47b06522"],"layout":"IPY_MODEL_2e79065ad70d4e16ab3f26f8b316c206"}},"d64be872bbd9472aa04749f836cceeb2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95163b94c3434d608323e06072a9bcad","placeholder":"​","style":"IPY_MODEL_96943dff03da4207ac86dcc3e5af2226","value":"vocab.txt: 100%"}},"9fba97b7f28743029128d9e0681fcf9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4096728ce3f342379227a75a711fcc86","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf91ee7d1ff941269133ec986888cae7","value":231508}},"e5183b1f75ba4a03b4f4372a47b06522":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f7f8f338b524461851a98026e6362c1","placeholder":"​","style":"IPY_MODEL_c9822ad50bdc4a9581bf037512c6d21d","value":" 232k/232k [00:00&lt;00:00, 1.42MB/s]"}},"2e79065ad70d4e16ab3f26f8b316c206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95163b94c3434d608323e06072a9bcad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96943dff03da4207ac86dcc3e5af2226":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4096728ce3f342379227a75a711fcc86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf91ee7d1ff941269133ec986888cae7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f7f8f338b524461851a98026e6362c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9822ad50bdc4a9581bf037512c6d21d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a2f2e7e5b5645ad9ee6b3affffe949a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7648868f4664663953829b3240426d0","IPY_MODEL_a73752a3046f48ccb43a2184dc536387","IPY_MODEL_6f1380f50b9545bfbd2f4bf3f261d144"],"layout":"IPY_MODEL_530a5aa26308495cba06a6fbdb6471e9"}},"c7648868f4664663953829b3240426d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51835a66dec44b8d97fe42052394ed51","placeholder":"​","style":"IPY_MODEL_341e41ee4228421b8900b2f7cbe8438b","value":"tokenizer.json: 100%"}},"a73752a3046f48ccb43a2184dc536387":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3104fc94e2954b54838cab815e28839c","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ea55bfb578340648a4b9dbe5a84d49f","value":466062}},"6f1380f50b9545bfbd2f4bf3f261d144":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6c4a5caa9c545809b5ca07299b1132f","placeholder":"​","style":"IPY_MODEL_7e4d71f485e44694ae2adbe53b9e8d42","value":" 466k/466k [00:00&lt;00:00, 2.83MB/s]"}},"530a5aa26308495cba06a6fbdb6471e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51835a66dec44b8d97fe42052394ed51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"341e41ee4228421b8900b2f7cbe8438b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3104fc94e2954b54838cab815e28839c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ea55bfb578340648a4b9dbe5a84d49f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6c4a5caa9c545809b5ca07299b1132f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e4d71f485e44694ae2adbe53b9e8d42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28fdc23910894252ae9a77b0411a2d61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32d4b6586b004dd7bf87867995e8e6d7","IPY_MODEL_1e6fea7006814d7d82cf3d35cf6c73a5","IPY_MODEL_642ef5fd68b1456a9fc3dd2307fae90e"],"layout":"IPY_MODEL_a2ec60051ca04e9b884181dfdcd89654"}},"32d4b6586b004dd7bf87867995e8e6d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8485de1584b24c648d459a5509c75e25","placeholder":"​","style":"IPY_MODEL_02891442cc044b5e98317319d03a7645","value":"config.json: 100%"}},"1e6fea7006814d7d82cf3d35cf6c73a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cffff082fbd94abbbd730c84f91a17bf","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eabe0fc0ed274aeabe906526a5830227","value":570}},"642ef5fd68b1456a9fc3dd2307fae90e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58efb67e259c4a9e88e29e24af93f40f","placeholder":"​","style":"IPY_MODEL_b5ec0679815243fb9b0d9c3ed0495669","value":" 570/570 [00:00&lt;00:00, 86.2kB/s]"}},"a2ec60051ca04e9b884181dfdcd89654":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8485de1584b24c648d459a5509c75e25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02891442cc044b5e98317319d03a7645":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cffff082fbd94abbbd730c84f91a17bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eabe0fc0ed274aeabe906526a5830227":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58efb67e259c4a9e88e29e24af93f40f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5ec0679815243fb9b0d9c3ed0495669":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# BERT(Bidirectional Encoder Representations from Transformers)"],"metadata":{"id":"jQs2AXUQxNAg"}},{"cell_type":"markdown","source":["# NLP에서의 사전 훈련(Pre-training)\n","- 2018년 딥 러닝 연구원 세바스찬 루더는 사전 훈련된 언어 모델의 약진을 보며 다음과 같은 말을 했습니다.\n","\n","\n","- > \"사전 훈련된 단어 임베딩이 모든 NLP 실무자의 도구 상자에서 사전 훈련된 언어 모델로 대체되는 것은 시간 문제이다.\"\n","\n","- BERT(Bidirectional Encoder Representations from Transformers)와 같은 트랜스포머 계열의 모델들이 자연어 처리를 지배했던 19년과 20년을 회고하면 이 말은 이미 현실이 되었습니다.\n","- BERT를 배우기에 앞서 워드 임베딩에서부터 ELMo, 그리고 트랜스포머에 이르기까지 자연어 처리가 발전되어온 흐름을 정리해봅시다."],"metadata":{"id":"w18kAYEixM-G"}},{"cell_type":"markdown","source":["## 사전 훈련된 워드 임베딩\n","\n","- 워드 임베딩 방법론들\n","  - Word2Vec, FastText, GloVe...\n","  \n","- 어떤 태스크를 수행할 때, 임베딩을 사용하는 방법으로는 크게 두 가지가 있다.\n","  1. 임베딩 층(Embedding layer)을 랜덤 초기화하여 **처음부터 학습**하는 방법.\n","  1. 방대한 데이터로 Word2Vec 등과 같은 임베딩 알고리즘으로 **'사전에 학습된'** 임베딩 벡터들을 가져와 사용하는 방법.\n"],"metadata":{"id":"KEZTge01xM7X"}},{"cell_type":"markdown","source":["- 문제점\n","  - 그런데 이 두 가지 방법 모두 하나의 단어가 하나의 벡터값으로 맵핑되므로,\n","  - 문맥을 고려하지 못 하여 다의어나 동음이의어를 구분하지 못하는 문제점이 있습니다.\n","  - 가령, 한국어에는 '사과'라는 단어가 존재하는데 이 '사과'는 용서를 빈다는 의미로도 쓰이지만, 먹는 과일의 의미로도 사용됩니다.\n","  - 그러나 임베딩 벡터는 '사과'라는 벡터에 하나의 벡터값을 맵핑하므로 이 두 가지 의미를 구분할 수 없었습니다.\n","\n","  - 이 한계는 **사전 훈련된 언어 모델**을 사용하므로서 극복할 수 있었으며\n","  - 아래에서 언급할 ELMo나 BERT 등이 이러한 문제의 해결책입니다.\n"],"metadata":{"id":"Dt83FSt1xM4n"}},{"cell_type":"markdown","source":["## 사전 훈련된 언어 모델\n","![](https://wikidocs.net/images/page/108730/image1.PNG)"],"metadata":{"id":"oK-Og0KLxMzp"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/108730/image2.PNG)"],"metadata":{"id":"iVoFE5kTxMwf"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/108730/image3.PNG)"],"metadata":{"id":"8_vcttCjxMue"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/108730/image4.PNG)"],"metadata":{"id":"mu_0r7FsxMrW"}},{"cell_type":"markdown","source":["### 사전 훈련된 언어 모델의 이점\n","- 컴퓨팅 비용과 탄소 배출량이 줄고,\n","- 모델을 처음부터 훈련시키는 데 필요한 시간과 리소스를 절약\n","- 다양한 분야의 태스크에 적용 가능"],"metadata":{"id":"kjaBecQPxMom"}},{"cell_type":"markdown","source":["## 마스크드 언어 모델(Masked Language Model, MLM)\n","\n","- 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 **마스킹(Masking)**합니다.\n","- **마스킹**이란 원래의 단어가 무엇이었는지 모르게 한다는 뜻.\n","\n","- 그리고 인공 신경망에게 이렇게 마스킹 된 단어들을(Masked words) '예측'하도록 함.\n","- 문장 중간에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 식입니다.\n","\n","- 예] ```'나는 [MASK]에 가서 그곳에서 빵과 [MASK]를 샀다'```를 주고 [MASK]에 들어갈 단어를 맞추게 합니다.\n"],"metadata":{"id":"zr6w7obIxMl4"}},{"cell_type":"markdown","source":["# 버트(Bidirectional Encoder Representations from Transformers, BERT)\n","![](https://media.gettyimages.com/id/97258631/photo/bert-and-elmo-prepare-for-the-show-in-a-dressing-room-backst.jpg?s=1024x1024&w=gi&k=20&c=ttFn_t6FuOjLL5ofEi5GgzFo2JsXQTBcQ8hfuOKFg_g=)\n","\n","- 2018년에 구글이 공개한 **사전 훈련된 모델**.\n","\n","- BERT라는 이름은 세서미 스트리트라는 미국 인형극의 케릭터 이름이기도 한데,\n","앞서 소개한 임베딩 방법론인 ELMo와 마찬가지로 세서미 스트리트의 케릭터 이름을 따온 것이기도 합니다.\n","\n","- BERT는 2018년에 공개되어 등장과 동시에 수많은 NLP 태스크에서 최고 성능을 보여주면서\n","명실공히 NLP의 한 획을 그은 모델로 평가받고 있습니다.\n"],"metadata":{"id":"CYDWZF4TxMg6"}},{"cell_type":"markdown","source":["## ▶ transformers 패키지\n","사전학습된 최첨단 모델들을 쉽게 다운로드하고 훈련시킬 수 있는 API와 도구를 제공\n","\n","- https://huggingface.co/docs/transformers\n","- https://huggingface.co/docs/transformers/ko/index  (한국어)"],"metadata":{"id":"eWfTeiETxMeC"}},{"cell_type":"markdown","source":["### ※ huggingface 란\n","![](https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.svg)\n","\n","https://huggingface.co/\n","\n","Hugging Face는 자연어 처리(NLP), 컴퓨터 비전(CV), 음성 처리 등 **다양한 머신 러닝 모델**을 쉽게 사용하고 배포할 수 있도록 지원하는 오픈소스 플랫폼이자 커뮤니티입니다.\n","\n","특히, **대규모 언어 모델(LLM) 및 전이 학습(Transfer Learning)** 프레임워크를 중심으로 한 도구와 서비스를 제공합니다.\n","\n","Hugging Face는 데이터 과학자, 연구자, 엔지니어들이 머신 러닝 모델을 쉽게 활용하고, 학습시키고, 배포할 수 있도록 도와주는 라이브러리와 클라우드 서비스를 제공합니다. 가장 유명한 라이브러리는 Transformers이며, 이는 NLP 및 다양한 AI 작업에 널리 사용됩니다."],"metadata":{"id":"xhZ8sGiV1OKk"}},{"cell_type":"markdown","source":["## 1. BERT 개요\n","- BERT는\n","  - 트랜스포머를 이용하여 구현됨,\n","  - 위키피디아(25억 단어)와 BooksCorpus(8억 단어)와 같은 레이블이 없는 텍스트 데이터로 '사전 훈련된 언어 모델'.\n","\n","- BERT의 성능이 높은 이유\n","  - '레이블이 없는 방대한 데이터'로 '사전 훈련'된 모델을 가지고,\n","  - 레이블이 있는 다른 작업(Task)에서 **'추가 훈련'**과 함께 **하이퍼파라미터를 재조정**하여\n","  - 이 모델을 사용하면 성능이 높게 나오는 기존의 사례들을 참고하였기 때문.\n"],"metadata":{"id":"ykRbMpYo1OH6"}},{"cell_type":"markdown","source":["## 파인 튜닝 Fine-tuning\n","다른 작업에 대해서 '파라미터 재조정'을 위한 '추가 훈련 과정'을 **파인 튜닝(Fine-tuning, 미세조정)**이라고 합니다.\n","\n","\n","![](https://wikidocs.net/images/page/35594/%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.PNG)\n","\n","- ↑위의 그림은 BERT의 파인 튜닝 예시입니다\n","\n","- 하고 싶은 태스크가 스팸 메일 분류라고 하였을 때,\n","\n","- 이미 위키피디아 등으로 '사전 학습된 BERT' 위에 '분류를 위한 신경망을 한 층 추가'합니다.\n","\n","- 이 경우, 비유하자면 BERT가 언어 모델 사전 학습 과정에서 얻은 지식을 활용할 수 있으므로\n","스팸 메일 분류에서 보다 더 좋은 성능을 얻을 수 있습니다.\n","\n","- 이전에 언급한 ELMo나 OpenAI GPT-1 등이 이러한 파인 튜닝 사례의 대표적인 예입니다.\n"],"metadata":{"id":"qzndiQ8Z1OD-"}},{"cell_type":"markdown","source":["## 2. BERT의 크기 (BERT-Base, BERT-Large)\n","\n","![](https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG)\n","\n","- BERT의 기본 구조는 트랜스포머의 '인코더'를 쌓아올린 구조.\n","  - 'Base 버전' 에서는 총 12개,\n","  - 'Large 버전'에서는 총 24개를 쌓았습니다.\n","\n","- Large 버전은 Base 버전보다\n","  - d_model의 크기나 셀프 어텐션 헤드(Self Attention Heads)의 수가 더 큽니다.\n","\n","- 트랜스포머 인코더 층의 수를 L, d_model의 크기를 D, 셀프 어텐션 헤드의 수를 A라고 하였을 때 각각의 크기는 다음과 같습니다.\n","\n","  - BERT-Base : L=12, D=768, A=12 : 110M개의 파라미터\n","  - BERT-Large : L=24, D=1024, A=16 : 340M개의 파라미터\n","\n","- ※ 아래 예제 에서는 편의를 위해 BERT-BASE를 기준으로 설명합니다.  "],"metadata":{"id":"aalpzWCw1OAF"}},{"cell_type":"markdown","source":["## 3. BERT의 문맥을 반영한 임베딩(Contextual Embedding)\n","\n","BERT는 문맥을 반영한 임베딩(Contextual Embedding)을 사용함\n","\n","![](https://wikidocs.net/images/page/115055/bert0.PNG)\n","\n","- BERT의 입력\n","  - 임베딩 층(Embedding layer)를 지난 임베딩 벡터들\n","  - d_model이 768\n","  - 768차원의 임베딩 벡터된 단어가 BERT의 입력\n","\n","- BERT의 출력\n","  - 내부 연산후\n","  - 동일하게 각 단어에 대해서 768차원의 벡터 출력"],"metadata":{"id":"2Hd84LI1xMbV"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC2.PNG)\n","\n","- BERT의 연산을 거친 후 출력 임베딩은 '문장의 문맥을 모두 참고'한 **문맥을 반영한 임베딩**이 됩니다.\n","\n","\n","[좌측 그림]에서\n","- [CLS]라는 벡터는 BERT의 초기 입력으로 사용되었을 입력 임베딩 당시에는 단순히 임베딩 층(embedding layer)를 지난 임베딩 벡터였지만,\n","- BERT를 지나고 나서는 [CLS], I, love, you라는 모든 단어 벡터들을 '모두 참고'한 후에 문맥 정보를 가진 벡터가 됩니다.\n","- ※ 모든 단어를 참고하고 있다는 것을 점선의 화살표로 표현하였습니다.\n","\n","\n","- 이는 [CLS]라는 단어 벡터 뿐만 아니라 다른 벡터들도 전부 마찬가지.\n","- 가령, 우측의 그림에서 출력 임베딩 단계의 love를 보면 BERT의 입력이었던\n","- 모든 단어들인 [CLS], I, love, you를 참고하고 있습니다.\n","\n","\n"],"metadata":{"id":"Q69vUoCGxMYT"}},{"cell_type":"markdown","source":["- 하나의 단어가 모든 단어를 참고하는 연산은 BERT 의 '12개 층'에서 전부 이루어지는 연산입니다\n","- 그리고 이를 12개의 층을 지난 후에 최종적으로 출력 임베딩을 얻게되는 것입니다\n","\n","- ↓ BERT 첫번째 층\n","\n","![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC3.PNG)\n","\n","- 첫번째 층의 출력임베딩은 ==> 두번째 층의 입력 ,임베딩이 된다.\n"],"metadata":{"id":"T1pw763txMV1"}},{"cell_type":"markdown","source":["BERT 가 '문맥을 반영한 출력 임베딩' 을 얻게 되는 이유!  => **셀프 어텐션**\n","\n","![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC4.PNG)\n","\n","- BERT 는 트랜스포머 인코더를 12번 쌓음\n","- 각 층마다 '멀티 헤드 셀프 어텐션'과 '포지션 와이즈 피드 포워드 신경망'을 수행"],"metadata":{"id":"NNnZqtu4xMSt"}},{"cell_type":"markdown","source":["## 4. BERT의 서브워드 토크나이저 : WordPiece\n","- BERT는 단어보다 더 작은 단위로 쪼개는 **서브워드 토크나이저**를 사용\n","- BERT가 사용한 토크나이저는 **WordPiece 토크나이저**\n","\n","- 서브워드 토크나이저 기본 아이디어!\n","  - 자주 등장하는 단어는 그대로 단어 집합에 추가\n","  - 자주 등장하지 않는 단어의 경우에는 더 작은 단위인 '서브워드'로 분리되어 서브워드들이 단어 집합에 추가\n","\n","- 이렇게 단어 집합이 만들어지고 나면, 이 단어 집합을 기반으로 토큰화를 수행  "],"metadata":{"id":"51f72pyyxMQB"}},{"cell_type":"markdown","source":["**BERT 에서 토크화 수행 방식**\n","\n","준비물 : 이미 훈련 데이터로부터 만들어진 단어 집합\n","\n","1. 토큰이 단어 집합에 존재한다.\n","  - → 해당 토큰을 분리하지 않는다.\n","\n","2. 토큰이 단어 집합에 존재하지 않는다.\n","  - → 해당 토큰을 서브워드로 분리한다.\n","  - → 해당 토큰의 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 **\"##\"**를 붙인 것을 토큰으로 한다.\n","\n","3. [예시]\n","  - ```embeddings``` 단어가 입력으로 들어왔을때\n","  - BERT의 단어 집합에 해당 단어가 존재하지 않는다면\n","  - 서브워드 토크나이저가 아니라면 OOV 문제 발생\n","  - 서브워드 토크나이저의 경우 해당 단어를 더 쪼개려고 시도합니다\n","  - 만약, BERT의 단어 집합에 ```em```, ```##bed```, ```##ding```, ```##s```라는 서브 워드들이 존재한다면\n","    - ```embeddings```는 ==>  ```em```, ```##bed```, ```##ding```, ```##s```로 분리됩니다.\n","\n","  - 여기서 **##** 은 이 서브워드들은 단어의 중간부터 등장하는 '서브워드'라는 것을 알려주기 위해 단어 집합 생성 시 표시해둔 기호입니다.\n","  - 이런 표시가 있어야만 ```em```, ```##bed```, ```##ding```, ```##s```를 다시 손쉽게 embeddings로 복원할 수 있을 것입니다.\n"],"metadata":{"id":"p-GM8a4ZxMKm"}},{"cell_type":"markdown","source":["### BertTokenizer"],"metadata":{"id":"Cl_lKW8_xMH4"}},{"cell_type":"code","source":["import pandas as pd\n","from transformers import BertTokenizer"],"metadata":{"id":"OzeiAVZOxMFE","executionInfo":{"status":"ok","timestamp":1763636753151,"user_tz":-540,"elapsed":7083,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274,"referenced_widgets":["3db2ab4d523043339c1c0e8e52bb0d4f","f7bebafb4317408a9b132e58e63ae24b","fb8d82bd26f24c449b6244e95cb567b8","0c31aec0f5054a3fbebc120660a8c99a","0dfc6c51ac824ca7bb2214fa6817305b","74f4bb6362464787a0355474f36f40fc","d3383a67f9224f1f98e5ddc7d55ae0b7","dc093a2dab53498198e48267f749ba1f","ffc3f7f549114888915ca29c7b6c9d3d","4e0fd61b6157476ba94b3f244c99e572","63257a0b8d67493fb2b37fdd36282b33","cbed109dfc1945519647ebaa885abbdd","d64be872bbd9472aa04749f836cceeb2","9fba97b7f28743029128d9e0681fcf9b","e5183b1f75ba4a03b4f4372a47b06522","2e79065ad70d4e16ab3f26f8b316c206","95163b94c3434d608323e06072a9bcad","96943dff03da4207ac86dcc3e5af2226","4096728ce3f342379227a75a711fcc86","bf91ee7d1ff941269133ec986888cae7","1f7f8f338b524461851a98026e6362c1","c9822ad50bdc4a9581bf037512c6d21d","8a2f2e7e5b5645ad9ee6b3affffe949a","c7648868f4664663953829b3240426d0","a73752a3046f48ccb43a2184dc536387","6f1380f50b9545bfbd2f4bf3f261d144","530a5aa26308495cba06a6fbdb6471e9","51835a66dec44b8d97fe42052394ed51","341e41ee4228421b8900b2f7cbe8438b","3104fc94e2954b54838cab815e28839c","4ea55bfb578340648a4b9dbe5a84d49f","e6c4a5caa9c545809b5ca07299b1132f","7e4d71f485e44694ae2adbe53b9e8d42","28fdc23910894252ae9a77b0411a2d61","32d4b6586b004dd7bf87867995e8e6d7","1e6fea7006814d7d82cf3d35cf6c73a5","642ef5fd68b1456a9fc3dd2307fae90e","a2ec60051ca04e9b884181dfdcd89654","8485de1584b24c648d459a5509c75e25","02891442cc044b5e98317319d03a7645","cffff082fbd94abbbd730c84f91a17bf","eabe0fc0ed274aeabe906526a5830227","58efb67e259c4a9e88e29e24af93f40f","b5ec0679815243fb9b0d9c3ed0495669"]},"id":"p-BVRfOd8TFk","executionInfo":{"status":"ok","timestamp":1763636816341,"user_tz":-540,"elapsed":8274,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"031bb788-04b3-44cc-e77c-18d9e70cd026"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db2ab4d523043339c1c0e8e52bb0d4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbed109dfc1945519647ebaa885abbdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a2f2e7e5b5645ad9ee6b3affffe949a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28fdc23910894252ae9a77b0411a2d61"}},"metadata":{}}]},{"cell_type":"code","source":["result = tokenizer.tokenize(\"Here is the sentence I want embeddings for.\")\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7p-FsxQ8gg-","executionInfo":{"status":"ok","timestamp":1763636873932,"user_tz":-540,"elapsed":14,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"b6fc594b-0c1c-457d-b784-aca22767f163"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.']\n"]}]},{"cell_type":"code","source":["tokenizer.vocab['here']   # BERT 의 단어집합에 특정단어가 있는지 조회 가능."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOcV-Zb88wl4","executionInfo":{"status":"ok","timestamp":1763636933256,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"91b4f82c-7fd8-4255-f28b-b48e6f6ca87a"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2182"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# tokenizer.vocab['embeddings']   # 없는 단어는 KeyError"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"id":"FDrcP9at8_Ec","executionInfo":{"status":"error","timestamp":1763636985617,"user_tz":-540,"elapsed":61,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"8db7af8f-dbfa-42d7-ea32-55dc1f29eea8"},"execution_count":5,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'embeddings'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-942527399.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyError\u001b[0m: 'embeddings'"]}]},{"cell_type":"code","source":["for token in result:\n","  print(token, tokenizer.vocab[token])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ou-4s73f8S_W","executionInfo":{"status":"ok","timestamp":1763637065639,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"06a65754-08eb-4401-e258-271ffac3b486"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["here 2182\n","is 2003\n","the 1996\n","sentence 6251\n","i 1045\n","want 2215\n","em 7861\n","##bed 8270\n","##ding 4667\n","##s 2015\n","for 2005\n",". 1012\n"]}]},{"cell_type":"code","source":["# BERT 의 단어집합 전체를 저장해보자\n","with open('vocabulary.txt', 'w') as f:\n","  for token in tokenizer.vocab.keys():\n","    f.write(token + '\\n')"],"metadata":{"id":"algi4BZS8S3i","executionInfo":{"status":"ok","timestamp":1763637211271,"user_tz":-540,"elapsed":16,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["df = pd.read_fwf('vocabulary.txt', header=None)\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"C1HDfTtT8Szn","executionInfo":{"status":"ok","timestamp":1763637318163,"user_tz":-540,"elapsed":85,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"61f963dd-b4f2-411c-e9dc-9e93dfede819"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["               0\n","0          [PAD]\n","1      [unused0]\n","2      [unused1]\n","3      [unused2]\n","4      [unused3]\n","...          ...\n","30517        ##．\n","30518        ##／\n","30519        ##：\n","30520        ##？\n","30521        ##～\n","\n","[30522 rows x 1 columns]"],"text/html":["\n","  <div id=\"df-3013ea33-34e9-48eb-a2e7-806d436e953d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[PAD]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[unused0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[unused1]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[unused2]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[unused3]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>30517</th>\n","      <td>##．</td>\n","    </tr>\n","    <tr>\n","      <th>30518</th>\n","      <td>##／</td>\n","    </tr>\n","    <tr>\n","      <th>30519</th>\n","      <td>##：</td>\n","    </tr>\n","    <tr>\n","      <th>30520</th>\n","      <td>##？</td>\n","    </tr>\n","    <tr>\n","      <th>30521</th>\n","      <td>##～</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>30522 rows × 1 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3013ea33-34e9-48eb-a2e7-806d436e953d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3013ea33-34e9-48eb-a2e7-806d436e953d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3013ea33-34e9-48eb-a2e7-806d436e953d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-6c6a35be-ce08-48a4-9199-018d05cd04c6\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c6a35be-ce08-48a4-9199-018d05cd04c6')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-6c6a35be-ce08-48a4-9199-018d05cd04c6 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_1cd0c38a-5a13-44c3-bfaa-df53ad82b766\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_1cd0c38a-5a13-44c3-bfaa-df53ad82b766 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 30522,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29929,\n        \"samples\": [\n          \"##fi\",\n          \"dramas\",\n          \"hut\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# 단어집합의 크기\n","len(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVmVroh0-dBs","executionInfo":{"status":"ok","timestamp":1763637342743,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"9142dec7-67be-4cba-df08-4e1d7932b3a1"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30522"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["df.loc[4667].values[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"S6oXv1hD8Sum","executionInfo":{"status":"ok","timestamp":1763637388334,"user_tz":-540,"elapsed":20,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"6f0321fb-d2ba-4fde-9c26-cfa2e3cc2018"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'##ding'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["참고] BERT에서 사용되는 특별 토큰들과 정숫값\n","\n","- **[PAD] - 0**     padding token\n","- **[UNK] - 100**   unknown token\n","- **[CLS] - 101**   classification token. 문장의 시작\n","- **[SEP] - 102**   separator token.  문장의 끝\n","- **[MASK] - 103**   mask token  (마스크언어모델 MLM 에서 사용)"],"metadata":{"id":"BzbwQQ-vxMCU"}},{"cell_type":"code","source":["df.loc[102].values[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0dD3va4wxL_N","executionInfo":{"status":"ok","timestamp":1763637518129,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}},"outputId":"1b0fbba1-d8e1-4e4a-c3c7-0f58ddac80d6"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## Position Embedding"],"metadata":{"id":"JxiAhOlcxL7p"}},{"cell_type":"markdown","source":["- 트랜스포머에서는 sin, cos 함수를 사용하여 포지셔널 인코딩(Positional Encoding)이라는 방법을 통해 단어의 위치 정보를 표현했습니다. 포지셔널 인코딩은 사인 함수와 코사인 함수를 사용하여\n","위치에 따라 다른 값을 가지는 행렬을 만들어 이를 단어 벡터들과 더하는 방법입니다.\n","\n","- BERT 에선 **학습**을 통해서 얻는 **포지션 임베딩(Position Embedding)**이라는 방법을 사용하여 단어의 위치 정보를 표현함.\n","\n","- 포지션 임베딩 (예시 그림) : 2개의 임베딩 층 사용\n","  - 단어의 임베딩 벡터\n","  - 포지션 임베딩 벡터\n","\n","![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC5.PNG)\n","\n","\n","[위 그림 설명]\n","\n","- WordPiece Embedding은\n","우리가 이미 알고 있는 단어 임베딩으로 실질적인 '입력'입니다.\n","\n","- 그리고 이 입력에 포지션 임베딩을 통해서 '위치 정보'를 더해주어야 합니다.\n","\n","- 포지션 임베딩의 아이디어는 굉장히 간단한데,\n","위치 정보를 위한 임베딩 층(Embedding layer)을 하나 더 사용합니다.\n","\n","- 가령, 문장의 길이가 4라면 4개의 포지션 임베딩 벡터를 학습시킵니다.\n","\n","- 그리고 BERT의 입력마다 다음과 같이 포지션 임베딩 벡터를 더해주는 것입니다.\n","\n","  - 첫번째 단어의 임베딩 벡터 + 0번 포지션 임베딩 벡터\n","  - 두번째 단어의 임베딩 벡터 + 1번 포지션 임베딩 벡터\n","  - 세번째 단어의 임베딩 벡터 + 2번 포지션 임베딩 벡터\n","  - 네번째 단어의 임베딩 벡터 + 3번 포지션 임베딩 벡터\n","\n","- 실제 BERT에서는 문장의 최대 길이를 512로 하고 있으므로, 총 512개의 포지션 임베딩 벡터가 학습됩니다.\n","\n","- 결론적으로 현재 설명한 내용을 기준으로는 BERT에서는 총 두 개의 임베딩 층이 사용됩니다.\n","  - 단어 집합의 크기가 30,522개인 단어 벡터를 위한 임베딩 층과\n","  - 문장의 최대 길이가 512이므로 512개의 포지션 벡터를 위한 임베딩 층입니다.\n","\n","- 사실 BERT는 세그먼트 임베딩(Segment Embedding)이라는 1개의 임베딩 층을 더 사용합니다.\n","이에 대해서는 뒤에 언급합니다...\n"],"metadata":{"id":"_s9LRoowxFvX"}},{"cell_type":"markdown","source":["## Bert 의 사전훈련 방식 2가지\n","Pre-training"],"metadata":{"id":"T9NTuiCZxFqh"}},{"cell_type":"markdown","source":["### BERT, GPT-1, ELMo <= 자연어 처리 분야에서 획기적인 변화를 몰고온 모델\n","\n","![](https://wikidocs.net/images/page/35594/bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png)\n","---\n","\n","### **1. BERT (Bidirectional Encoder Representations from Transformers)**\n","- **발표**: 2018년 (Google)\n","- **핵심 아이디어**:\n","  - 트랜스포머의 **인코더 구조**를 사용하며, 문장의 양방향 문맥을 동시에 학습.\n","  - 두 가지 사전 학습 태스크를 사용:\n","    1. **Masked Language Model (MLM)**: 일부 단어를 가리고 이를 예측.\n","    2. **Next Sentence Prediction (NSP)**: 두 문장이 연결되는지 판단.\n","- **특징**:\n","  - 양방향 문맥 표현으로 단어의 풍부한 의미를 학습.\n","  - 다양한 NLP 태스크에서 state-of-the-art 성능 달성.\n","- **장점**:\n","  - ELMo보다 더 정교한 문맥 정보를 제공.\n","  - 양방향 학습으로 문장 내의 더 깊은 관계를 파악 가능.\n","- **한계**:\n","  - 사전 학습 비용이 매우 큼(대규모 데이터와 컴퓨팅 자원 필요).\n","---\n","\n","### **2. GPT-1 (Generative Pre-trained Transformer 1)**\n","- **발표**: 2018년 (OpenAI)\n","- **핵심 아이디어**:\n","  - 트랜스포머 아키텍처를 사용하여 **사전 학습(pre-training)**과 **미세 조정(fine-tuning)** 개념을 도입했습니다.\n","  - 언어 모델링(다음 단어 예측)을 통해 대규모 데이터에서 사전 학습을 수행한 후, 특정 태스크에 맞게 미세 조정.\n","- **특징**:\n","  - 트랜스포머의 **디코더 구조**를 사용.\n","  - 단방향 언어 모델(이전 토큰만 참조 가능).\n","- **장점**:\n","  - 전이 학습(transfer learning)의 효과를 보여줌.\n","  - 당시 기준으로 여러 NLP 태스크에서 높은 성능 달성.\n","- **한계**:\n","  - 단방향 특성으로 인해 문맥을 완전히 활용하지 못함.\n","\n","### **3. ELMo (Embeddings from Language Models)**\n","- **발표**: 2018년 (AllenNLP 팀)\n","- **핵심 아이디어**:\n","  - ELMo는 단어의 의미를 문맥에 따라 동적으로 표현합니다. 이전에는 단어를 고정된 벡터(Word2Vec, GloVe 등)로 표현했지만, ELMo는 **문맥에 따라 단어 임베딩이 달라지는 방식**을 도입했습니다.\n","  - 양방향 LSTM을 사용하여 문장의 양방향 정보를 활용합니다.\n","- **장점**:\n","  - \"bank\" 같은 단어의 다의성을 문맥을 통해 구별할 수 있음.\n","  - 다양한 NLP 태스크(문장 분류, 개체명 인식 등)에 쉽게 적용 가능.\n","- **한계**:\n","  - LSTM 기반이라 대규모 데이터를 처리하는 데 시간이 오래 걸림.\n","\n","\n","  \n","\n","\n","---\n","\n","### **비교 요약**\n","| 모델   | 구조                     | 방향성        | 주요 특징                          |\n","|--------|--------------------------|---------------|-------------------------------------|\n","| BERT   | 트랜스포머 인코더        | 양방향       | MLM, NSP 사용, 문맥 이해에 강점     |\n","| GPT-1  | 트랜스포머 디코더        | 단방향       | 전이 학습 도입, 생성 중심          |\n","| ELMo   | LSTM                    | 양방향       | 문맥에 따른 동적 임베딩            |\n","\n","이 모델들은 이후의 GPT-3, T5, ChatGPT와 같은 발전된 모델들의 기반이 되었습니다."],"metadata":{"id":"QtrYDJWrxFoM"}},{"cell_type":"markdown","source":["BERT의 사전 훈련 방법  두 가지로 나뉜다\n","- 첫번째 **마스크드 언어 모델(MLM, Masked Language Model)**\n","- 두번째 **다음 문장 예측(NSP, Next sentence prediction)**"],"metadata":{"id":"a_XimcHgxFle"}},{"cell_type":"markdown","source":["### 1) 마스크드 언어 모델(Masked Language Model, MLM)\n","\n","- 사전 훈련을 위해서 인공 신경망의 입력으로 들어가는 입력 텍스트의 15%의 단어를 랜덤으로 마스킹(Masking) 함\n","- 그리고 인공 신경망에게 이 가려진 단어들을(Masked words) 예측하도록 합니다"],"metadata":{"id":"1hNlKUwkxFiw"}},{"cell_type":"code","source":["# '나는 [MASK]에 가서 그곳에서 빵과 [MASK]를 샀다'\n","#   => '슈퍼'와 '우유'를 맞추게 합니다."],"metadata":{"id":"-FRRsv07xFgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전부 [MASK]로 변경하지는 않고, 랜덤으로 선택된 15%의 단어들은\n","# 다시 다음과 같은 비율로 규칙이 적용됩니다.\n","\n","# 80%의 단어들은 [MASK]로 변경한다.\n","# Ex) The man went to the store → The man went to the [MASK]\n","\n","# 10%의 단어들은 랜덤으로 단어가 변경된다.\n","# Ex) The man went to the store → The man went to the dog\n","\n","# 10%의 단어들은 동일하게 둔다.\n","# Ex) The man went to the store → The man went to the store"],"metadata":{"id":"2ZwVTBF9xFdU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/115055/%EC%A0%84%EC%B2%B4%EB%8B%A8%EC%96%B4.PNG)\n","\n","- 전체 단어의 85% 학습에 사용되지 않음\n","- 전체 단어의 15% 만 학습에 사용됨\n","  - 12% 는 [MASK]로 변경 후에 원래 단어를 예측\n","  - 1.5%는 랜덤으로 단어가 변경된 후에 원래 단어를 예측\n","  - 1.5%는 단어가 변경되지는 않았지만,\n","BERT는 이 단어가 변경된 단어인지 원래 단어가 맞는지는 알 수 없슴.\n","    - 이 경우에도 BERT는 원래 단어가 무엇인지를 예측하도록 함"],"metadata":{"id":"X-0uU3hRxFa5"}},{"cell_type":"markdown","source":["**[예시]**\n","\n","- 문장 : 'My dog is cute. he likes playing'\n","\n","- 전처리, BERT 의 서브워드 토크나이징 후 : ['my', 'dog', 'is' 'cute', 'he', 'likes', 'play', '##ing']\n","\n","- 위 토크화된 결과를  BERT 의 입력으로 사용"],"metadata":{"id":"TTrIh4O1xEaG"}},{"cell_type":"markdown","source":["- ↓ 언어모델 학습을 위해 다음과 같이 데이터가 변경되었다고 가정\n","- ↓ 'dog' 토큰은 [MASK]로 변경\n","- ↓ BERT 모델이 원래 단어를 맞추려고 하는 모습\n","\n","![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC8.PNG)\n","\n","- 오직 'dog' 위치의 출력층의 벡터만이 사용됨\n","- 다른 위치의 벡터들은 예측과 학습에 사용되지 않음\n","- 구체적으로는 BERT의 손실 함수에서 다른 위치에서의 예측은 무시\n","- 출력층에서는 예측을 위해 단어 집합의 크기만큼의 밀집층(Dense layer)에\n","소프트맥스 함수가 사용된 1개의 층을 사용하여 원래 단어가 무엇인지를 맞추게 됨\n","\n","\n"],"metadata":{"id":"dd39StjPxEXt"}},{"cell_type":"markdown","source":["- ↓ 다음과 같이 데이터셋이 변경되었다면 어떨까?\n","\n","  - 'dog' 토큰은 [MASK]로 변경.\n","  - 'he'는 랜덤 단어 'king'으로 변경.\n","  - 'play'는 변경되진 않았지만 예측에 사용됨.\n","\n","![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC9.PNG)"],"metadata":{"id":"KXb71rdZxEU9"}},{"cell_type":"markdown","source":["### 2) 다음 문장 예측(Next Sentence Prediction, NSP)\n","\n","- BERT는 **두 개의 문장** 을 준 후에\n","이 문장이 **'이어지는 문장'인지 아닌지**를 맞추는 방식으로 훈련\n","\n","- 입력\n","  - 실제 이어지는 두개의 문장 50%\n","  - 램덤으로 이어붙인 두개의 문장 50%\n","- label\n","  - 이어지는 문장 여부\n","\n","\n","- ● 이어지는 문장의 경우\n","  - Sentence A : The man went to the store.\n","  - Sentence B : He bought a gallon of milk.\n","  - Label = IsNextSentence\n","\n","- ● 이어지는 문장이 아닌 경우 경우\n","  - Sentence A : The man went to the store.\n","  - Sentence B : dogs are so cute.\n","  - Label = NotNextSentence  "],"metadata":{"id":"WTAb4b_cCgCM"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC10.PNG)\n","\n","- **[SEP]** 토큰 : BERT의 입력시 문장의 끝에\n"," [SEP] 토큰을 사용하여 문장을 구분.  \n","\n","- **[CLS]** 토큰 : 이 두 문장이 실제 '이어지는 문장인지 아닌지'를 [CLS] 토큰의 위치의 출력층에서 이진 분류 문제로 풀도록 합니다\n","\n","- '마스크드 언어 모델'과 '다음 문장 예측'은\n","따로 학습하는 것이 아닌 loss를 합하여 **학습이 '동시에 이루어집니다'**.\n","\n","- BERT가 NSP를 학습하는 이유\n","  - BERT 풀고자 하는 태스크 중에서는 QA(Question Answering)나 NLI(Natural Language Inference)와 같이 **두 문장의 관계**를 이해하는 것이 중요한 태스크들이 있기 때문.\n"],"metadata":{"id":"7DdEA_TGCf_e"}},{"cell_type":"markdown","source":["## 7. 세그먼트 임베딩(Segment Embedding)\n","\n","![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC7.PNG)\n","\n","- BERT는 QA 등과 같은 **두 개의 문장 입력**이 필요한 태스크를 풀기도 합니다\n","- BERT가 '구분' 하는 '문장' 이라는 것은 [SEP] 토큰으로 구분되는 문장을 의미하는 것은 아닙니다.  (다음에 설명)\n","\n","- Segment Embedding\n","  - **'문장구분'** 을 위해 BERT 에서 사용하는 또 다른 임베딩 층 (Embedding layer)\n","  - 첫번째 문장에는 Sentence 0 임베딩,\n","두번째 문장에는 Sentence 1 임베딩을 더해주는 방식.\n","  - 임베딩 벡터는 두 개만 사용됨\n","    1. Segment A (혹은 Segment ID: 0)\n","      - 첫번째 문장에 해당하는 토큰들\n","    1. Segment B (혹은 Segment ID: 1)\n","      - 두번째 문장에 해당하는 토큰들\n","\n","- 결론적으로 BERT는 총 3개의 임베딩 층이 사용됩니다.\n","\n","  - WordPiece Embedding : 실질적인 입력이 되는 워드 임베딩. 임베딩 벡터의 종류는 단어 집합의 크기로 30,522개.\n","  - Position Embedding : 위치 정보를 학습하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 길이인 512개.\n","  - Segment Embedding : 두 개의 문장을 구분하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수인 2개."],"metadata":{"id":"hyAnrdFmCf77"}},{"cell_type":"markdown","source":["**주의! 위에서 말하는 BERT의 입력 '문장' 이란**\n","\n","- 단순히 [SEP] 토큰으로 나뉘는 문장의 의미가 아니다\n","\n","- ex) [Q,A] 두 종류의 텍스트로 입력 받아도\n","  - 두 개의 문장은 실제로 두 종류의 텍스트, 두개의 문서 일수도 있다"],"metadata":{"id":"pKKZ-Sx6Cf4C"}},{"cell_type":"markdown","source":["## BERT 를 파인 튜닝 하기\n","fine-tuning (미세조정)"],"metadata":{"id":"HZEpRC5jCf0T"}},{"cell_type":"markdown","source":["### 1) 하나의 텍스트에 대한 텍스트 분류 유형(Single Text Classification)\n","![](https://wikidocs.net/images/page/115055/apply1.PNG)\n","\n","- 영화 리뷰 감성 분류, 로이터 뉴스 분류 등과 같이 입력된 문서에 대해서 '분류'를 하는 유형\n","- 문서의 시작에 [CLS] 라는 토큰을 입력\n","  - [CLS] 토큰은 BERT가 분류 문제를 풀기위한 특별한 토큰! → 이는 BERT를 실질적으로 사용하는 단계인 파인 튜닝 단계에서도 마찬가지입니다.\n","\n","- 텍스트 분류 문제를 풀기 위해서 [CLS] 토큰의 위치의 출력층에서 밀집층(Dense layer),\n","(aka. 완전 연결층(fully-connected layer))을 추가하여 분류에 대한 예측을 하게됩니다"],"metadata":{"id":"sfyaSwi3CfaU"}},{"cell_type":"markdown","source":["### 2) 하나의 텍스트에 대한 태깅 작업(Tagging)\n","![](https://wikidocs.net/images/page/115055/apply2.PNG)\n","\n","- 태깅작업 예\n","  - 각 단어에 품사를 태깅하는 '품사 태깅'\n","  - 개체를 태깅하는 '개체명 인식 작업 (NER: Named Entity Recognition)'\n","\n","- 출력층에서는 입력 텍스트의 각 토큰의 위치에 밀집층을 사용하여 분류에 대한 예측 수행.  "],"metadata":{"id":"577vs7ioCfL_"}},{"cell_type":"markdown","source":["### 3) 텍스트의 쌍에 대한 분류 또는 회귀 문제(Text Pair Classification or Regression)\n","![](https://wikidocs.net/images/page/115055/apply3.PNG)\n","\n","- 텍스트의 쌍(Text pair)을 입력받는 태스크\n","- 대표적인 예 => 자연어 추론 (NLI. Natural language inference)\n","\n","  - 두 문장이 주어졌을 때, 하나의 문장이 다른 문장과 논리적으로 어떤 관계에 있는지를 분류하는 것\n","  - 분류 유형\n","    - 모순 관계(contradiction)\n","    - 함의 관계(entailment)\n","    - 중립 관계(neutral)"],"metadata":{"id":"T19aNtRHCfJJ"}},{"cell_type":"markdown","source":["### 4) 질의 응답(Question Answering)\n","![](https://wikidocs.net/images/page/115055/apply4.PNG)\n","\n","- QA(Question Answering) 태스크\n","- '질문' 과 '본문' 두개의 텍스트의 쌍을 입력\n","\n","- [예시]\n","  - '질문' : \"강우가 떨어지도록 영향을 주는 것은?\"\n","  - '본문' : \"기상학에서 강우는 대기 수증기가 응결되어 중력의 영향을 받고 떨어지는 것을 의미합니다. 강우의 주요 형태는 이슬비, 비, 진눈깨비, 눈, 싸락눈 및 우박이 있습니다.\"\n","  \n","  - '정답' : '중력'\n"],"metadata":{"id":"29z7pxjACfGe"}},{"cell_type":"markdown","source":["## 9. 그 외 기타\n","- 훈련 데이터는 위키피디아(25억 단어)와 BooksCorpus(8억 단어) ≈ 33억 단어\n","- WordPiece 토크나이저로 토큰화를 수행 후 15% 비율에 대해서 마스크드 언어 모델 학습\n","- 두 문장 Sentence A와 B의 합한 길이. 즉, 최대 입력의 길이는 512로 제한\n","- 100만 step 훈련 ≈ (총 합 33억 단어 코퍼스에 대해 40 에포크 학습)\n","- 옵티마이저 : 아담(Adam)\n","- 학습률(learning rate) : $10^{-4}$\n","- 가중치 감소(Weight Decay) : L2 정규화로 0.01 적용\n","- 드롭 아웃 : 모든 레이어에 대해서 0.1 적용\n","- 활성화 함수 : relu 함수가 아닌 gelu 함수\n","- 배치 크기(Batch size) : 256"],"metadata":{"id":"m6xo8GacCfDv"}},{"cell_type":"markdown","source":["## 10. 어텐션 마스크(Attention Mask)\n","![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC11.PNG)\n","\n","- BERT를 실제로 실습하게 되면 **어텐션 마스크** 라는 시퀀스 입력이 추가로 필요\n","- BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분할 수 있도록 알려주는 입력\n","- 0, 1 두가지 값을 가짐\n","  - 1 은 해당 토큰은 실제 단어이므로 마스킹을 하지 않는다라는 의미\n","  - 0 은 해당 토큰은 패딩 토큰이므로 마스킹을 한다는 의미\n"],"metadata":{"id":"miql4jtZCfBX"}},{"cell_type":"markdown","source":["# [참고] transformer 계열의 사전훈련모델 들\n","\n","Transformer 계열의 사전훈련 모델들은 자연어 처리(NLP) 및 기타 작업에서 다양한 목적으로 설계된 모델들로, 대표적인 것들은 다음과 같습니다:\n","\n","---\n","\n","### **1. BERT 계열 (Bidirectional Encoder Representations from Transformers)**\n","- **BERT (2018, Google)**  \n","  양방향으로 문맥을 이해하는 Transformer Encoder 기반의 모델로, Masked Language Model(MLM)과 Next Sentence Prediction(NSP) 방식으로 학습.  \n","  - 예: BERT-base, BERT-large\n","  - 활용: 텍스트 분류, 질의응답, 문장 관계 판단\n","\n","- **RoBERTa (2019, Facebook)**  \n","  BERT의 변형으로, NSP를 제거하고 더 많은 데이터와 긴 학습시간으로 성능 향상.  \n","  - 예: RoBERTa-base, RoBERTa-large\n","\n","- **ALBERT (2019, Google)**  \n","  파라미터를 감소시키는 기법(예: Factorized Embedding Parameterization)을 적용해 더 효율적.  \n","  - 활용: 효율적인 파라미터 사용, NLP 태스크\n","\n","- **DistilBERT (2019, Hugging Face)**  \n","  BERT를 경량화한 모델로, 계산 비용이 적으면서도 성능 유지.  \n","  - 활용: 실시간 시스템\n","\n","---\n","\n","### **2. GPT 계열 (Generative Pre-trained Transformers)**\n","- **GPT (2018, OpenAI)**  \n","  순방향 언어 생성을 목표로 하는 Transformer Decoder 기반의 모델.  \n","  - GPT, GPT-2, GPT-3, GPT-4 등이 점진적으로 발전.\n","  - 활용: 텍스트 생성, 요약, 번역, 대화형 AI\n","\n","- **ChatGPT (OpenAI)**  \n","  GPT-3.5, GPT-4를 기반으로 한 대화 모델.  \n","  - 활용: 대화형 AI, 고객지원, 콘텐츠 생성\n","\n","- **CodeX (OpenAI)**  \n","  코딩 특화 GPT 모델로, 코드 생성과 디버깅에 활용.\n","\n","---\n","\n","### **3. T5 계열 (Text-to-Text Transfer Transformer)**\n","- **T5 (2019, Google)**  \n","  모든 NLP 태스크를 텍스트-텍스트 변환 문제로 통합.  \n","  - 예: T5-small, T5-base, T5-large\n","  - 활용: 번역, 문장 요약, 질의응답\n","\n","- **mT5 (2021, Google)**  \n","  다국어에 특화된 T5 모델.\n","\n","- **Flan-T5 (Google)**  \n","  지식 학습을 추가해 정밀도를 높인 버전.\n","\n","---\n","\n","### **4. XLNet 계열**\n","- **XLNet (2019, Google/CMU)**  \n","  BERT와 GPT의 장점을 결합한 모델로, Auto-Regressive 모델과 Auto-Encoding 모델의 혼합.  \n","  - 활용: 문맥 이해 및 생성 태스크\n","\n","---\n","\n","### **5. ELECTRA 계열**\n","- **ELECTRA (2020, Google)**  \n","  \"Generator-Discriminator\" 방식을 사용해 Masked Token 복원 대신, \"진짜 또는 가짜\" 토큰 판단.  \n","  - 활용: 효율적인 사전학습\n","\n","---\n","\n","### **6. 다국어 및 대규모 모델**\n","- **mBERT (Multilingual BERT)**  \n","  다국어 처리를 위해 설계된 BERT.\n","\n","- **XLM-R (2020, Facebook)**  \n","  RoBERTa를 기반으로 다국어 처리에 최적화.\n","\n","- **LLaMA (2023, Meta)**  \n","  연구 중심의 경량화 대규모 모델.\n","\n","---\n","\n","### **7. Vision-Language 및 기타 특화 모델**\n","- **BART (2019, Facebook)**  \n","  Seq2Seq 기반으로 문장 생성 및 요약에 강점.  \n","\n","- **DALL-E (OpenAI)**  \n","  텍스트-이미지 생성 모델.\n","\n","- **CLIP (OpenAI)**  \n","  텍스트와 이미지를 함께 학습하여 멀티모달 작업 수행.\n","\n","- **ViT (Vision Transformer, Google)**  \n","  Transformer를 컴퓨터 비전에 적용.\n","\n","---\n","\n","위의 모델들은 특정 태스크나 사용 사례에 따라 다양하게 변형되고 응용됩니다."],"metadata":{"id":"mHK4J_MECe-P"}},{"cell_type":"markdown","source":["# [참고] 사전훈련모델과 LLM 의 차이점은?\n","\n","사전훈련 모델(Pretrained Model)과 대규모 언어 모델(LLM, Large Language Model)은 밀접하게 관련이 있지만, 약간의 차이가 있습니다. 이를 구분하려면 두 개념의 정의와 목적을 이해해야 합니다.\n","\n","---\n","\n","### **1. 사전훈련 모델 (Pretrained Model)**\n","\n","#### **정의**  \n","- **사전훈련 모델**은 대규모 데이터셋을 기반으로 일반적인 언어 패턴을 학습한 모델을 말합니다. 특정 태스크(예: 감정 분석, 번역 등)에 적용되기 전에 미리 학습(pre-training)된 모델입니다.  \n","- 모델 크기와 관계없이, 사전훈련된 후 특정 작업(fine-tuning)이나 전이 학습(transfer learning)을 통해 최적화됩니다.\n","\n","#### **주요 특징**\n","- **목적**: 언어의 기본적인 구조와 의미를 학습.  \n","- **학습 방식**: 일반적으로 다음과 같은 방식으로 학습:\n","  - **Masked Language Modeling (MLM)**: BERT 계열\n","  - **Causal Language Modeling (CLM)**: GPT 계열\n","  - **Denoising Autoencoders**: BART 등\n","- **크기**: 작은 모델(BERT-base)부터 중간 크기(T5-large)까지 다양.  \n","- **사용 사례**: 특정 NLP 태스크에 특화된 모델로 발전.  \n","  - 예: 감정 분석, 질의응답, 텍스트 요약, 번역 등.\n","\n","#### **예시**\n","- BERT, RoBERTa, T5, GPT-2 등  \n","- BERT는 기본적으로 사전훈련 모델로 설계되어 특정 태스크에 맞춰 세밀한 조정(fine-tuning)을 거칩니다.\n","\n","---\n","\n","### **2. 대규모 언어 모델 (LLM, Large Language Model)**\n","\n","#### **정의**  \n","- **대규모 언어 모델(LLM)**은 사전훈련 모델의 한 유형이지만, **매우 큰 규모**(파라미터 수와 데이터 크기 기준)로 설계되어 강력한 언어 이해 및 생성 능력을 제공합니다.  \n","- LLM은 범용적이고, 사전학습 이후 추가적인 세밀한 조정을 거치지 않아도 다양한 태스크를 수행할 수 있습니다(Zero-shot 또는 Few-shot Learning).\n","\n","#### **주요 특징**\n","- **목적**:\n","  - 다양한 언어 태스크를 범용적으로 해결.\n","  - 특정 태스크 없이도(Zero-shot) 인간 수준의 언어 이해 및 생성을 목표로 함.\n","- **학습 방식**: 대규모 텍스트 데이터를 기반으로 CLM 또는 변형된 방법 사용.\n","- **크기**: 수십억~수조 개의 파라미터.\n","  - 예: GPT-3 (175억 파라미터), GPT-4, PaLM-2, LLaMA\n","- **추가 기능**:\n","  - 다중 태스크 수행 능력.\n","  - Zero-shot, Few-shot 학습.\n","  - 대화형 AI(예: ChatGPT).\n","\n","#### **예시**\n","- GPT-3, GPT-4, PaLM, Claude, LLaMA 등  \n","- ChatGPT는 GPT-3.5 또는 GPT-4를 기반으로 추가 미세 조정을 거친 LLM의 사례입니다.\n","\n","---\n","\n","### **차이점 비교**\n","\n","| **구분**                | **사전훈련 모델**                        | **대규모 언어 모델 (LLM)**               |\n","|-------------------------|-----------------------------------------|------------------------------------------|\n","| **규모**               | 크기 제한적 (수백~수십억 파라미터)       | 매우 크다 (수십억~수조 파라미터)         |\n","| **목적**               | 특정 NLP 태스크에 전이학습 사용          | 범용적인 언어 이해와 생성                |\n","| **학습 데이터**         | 일반적인 언어 데이터셋                  | 대규모 데이터셋 (웹, 책, 논문 등)        |\n","| **사용 방식**          | 주로 Fine-tuning 필요                   | Zero-shot, Few-shot 수행 가능            |\n","| **대표 모델**          | BERT, RoBERTa, T5                       | GPT-3, GPT-4, Claude, LLaMA              |\n","\n","---\n","\n","### **결론**\n","- 모든 **LLM**은 사전훈련된 모델입니다.  \n","  그러나 모든 사전훈련 모델이 LLM은 아닙니다.\n","  - **LLM**은 사전훈련 모델 중에서도 매우 큰 규모와 범용성을 갖춘 모델을 의미합니다.\n","  - 사전훈련 모델은 크기나 사용 목적에 따라 범용적이지 않을 수 있습니다.\n","\n","따라서, 사전훈련 모델은 LLM의 상위 개념이며, **LLM은 사전훈련 모델의 고도화된 형태**라고 볼 수 있습니다."],"metadata":{"id":"ejOGSGNJCe7e"}},{"cell_type":"code","source":[],"metadata":{"id":"zS7WsHoPCe4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-QGnpDGhCe2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YEdMrfNjCey4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ci-ULhopCeuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jRMpYtPjxESW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-4pSKiCxAEn"},"outputs":[],"source":[]}]}