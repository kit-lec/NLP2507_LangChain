{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO3V8Mfgly7g3UW+evErvtR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 트랜스포머 (Transformer)\n","* 2017년 구글브레인 이 발표한 논문인 **\"Attention is all you need\"** 에서 나온 모델\n","  - 논문링크 : https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n","* 트랜스포머는 <u>RNN을 사용하지 않고</u> '인코더'와 '디코더'를 설계하였으며, 성능도 RNN보다 우수함\n","  - 처음에는 자연어 처리 분야에서만 사용되었으나 이후 컴퓨터 비전 분야까지 확장됐으며 현재는 다양한 분야에서 채택되고 있다.\n","\n","* 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델\n","\n","* 이후 등장한 BERT, GPT, AlphaFold 2 등이 Transformer 기반으로 만들어졌다.\n","\n","![](https://i.namu.wiki/i/Og6EWpFFi7q01asl6LEay_-VVe9Vpedia1t2IAEXShjv2t_T2bzUdRMYa98XSfv6W_7S0_TWn9Ksncy6g6bCZM4zXS1I7QvfQO4xx9ke0yo1W8d6oYkicOKdv8XMlf_SUM_xGN3NnqYUvpYSz7nT0Q.webp)\n","\n"],"metadata":{"id":"idNfHKyFJlcN"}},{"cell_type":"markdown","source":["# 1. 기존의 seq2seq 모델의 한계\n","\n","![](https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG)\n","\n","- 기존의 Seq2Seq 는 인코더-디코더 구조로 구성 되었다\n","  - 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축하고\n","  - 디코더는 이 벡터 표현을 통해서 출력 시퀀스를 만들어냈다\n","- 단점!\n","  - 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서\n","입력 시퀀스의 정보가 일부 손실된다는 단점. (Vanishing Gradient)\n","  - 이를 보정하기 위해 '어텐션'이 사용되었었다.\n","  \n","- 트랜스포머의 아이디어!\n","  - **'어텐션'** 을 RNN 보정을 위한 용도로 사용하는 것이 아니라 **어텐션만으로 인코더와 디코더를** 만들어보면 어떨까?\n","\n"],"metadata":{"id":"0Mj1HGSbJlXJ"}},{"cell_type":"markdown","source":["# 2. 트랜스포머(Transformer)의 주요 하이퍼파라미터\n","\n","- 트랜스포머 논문에서 설정된 하이퍼 파라미터 값들.\n","  - 사용자가 모델 설계시 변경할수 도 있는 값들.\n","- 의미에 대해서는 뒤에서 설명예정"],"metadata":{"id":"_F4TUrCUJlUa"}},{"cell_type":"markdown","source":["---\n","$$d_{model} = 512$$\n","<br>\n","- 인코더와 디코더에서의 정해진 입력과 출력의 크기\n","- 임베딩 벡터의 차원 또한 $d_{model}$\n","- 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때에도 이 차원을 유지.\n","- 논문에서는 512입니다\n","\n","---\n","$$\\text{num_layers} = 6$$\n","\n","- 트랜스포머에서 하나의 **인코더와 디코더를 '층(layer)'**으로 생각할수 있다.\n","- 트랜스포머 모델에서 **인코더와 디코더가 총 몇개의 '층'**으로 구성되었는지를 의미.\n","- 논문에서는 인코더와 디코더를 각각 총 6개 쌓았다.\n","\n","\n","---\n","$$\\text{num_heads} = 8$$\n","\n","- 트랜스포머에서는 어텐션을 사용할 때, 한 번 하는 것 보다 여러 개로 분할해서 **병렬로 어텐션을 수행**하고\n","- 그 결과값(들)을 다시 하나로 합치는 방식을 사용한다.\n","- $\\text{num_heads}$ 값은 그 **병렬의 개수**를 의미합니다.\n","---\n","$$d_{ff} = 2048$$\n","\n","- 트랜스포머 내부에는 피드 포워드 신경망이 존재하며\n","- $d_{ff}$는 해당 신경망의 은닉층의 크기다.\n","- 피드 포워드 신경망의 입력층과 출력층의 크기는 $d_{model}$\n","입니다."],"metadata":{"id":"vzEXDWGUJlSE"}},{"cell_type":"markdown","source":["# 3. 트랜스포머"],"metadata":{"id":"lqu4AG_yJlPe"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/31379/transformer1.PNG)"],"metadata":{"id":"gKlIVLOuJlKQ"}},{"cell_type":"markdown","source":["- 트랜스포머는 RNN을 사용하지 않지만,\n","- 기존의 seq2seq처럼 **인코더-디코더 구조**를 유지한다\n","  - 인코더에서 입력 시퀀스를 입력\n","  - 디코더에서 출력 시퀀스를 출력\n","- 이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time step)을 가지는 구조였다면,\n","- 트랜스포머에서는 **인코더와 디코더라는 단위가 N개로 구성되는 구조**.\n","\n","- 논문에서는 인코더와 디코더의 개수를 각각 6개 사용."],"metadata":{"id":"gDdelB2fJlHn"}},{"cell_type":"markdown","source":["---\n","![](https://wikidocs.net/images/page/31379/transformer2.PNG)\n","- 위의 그림은 인코더와 디코더가 6개씩 존재하는 트랜스포머의 구조를 보여줍니다.\n","- ※ 인코더와 디코더가 각각 여러 개 쌓여있다는 의미를 사용할 때는 알파벳 s를 뒤에 붙여 encoders, decoders라고 표현하겠습니다."],"metadata":{"id":"i_qnHQLcJlFL"}},{"cell_type":"markdown","source":["---\n","![](https://wikidocs.net/images/page/31379/transformer4_final_final_final.PNG)\n","- 위의 그림은 인코더로부터 정보를 전달받아 디코더가 출력 결과를 만들어내는 트랜스포머 구조.\n","- **디코더**는\n","  - 마치 기존의 seq2seq 구조처럼 시작 심볼 &lt;sos&gt; 를 입력으로 받아\n","  - 종료 심볼 &lt;eos&gt;가 나올 때까지 연산을 진행합니다.\n","  - 이는 RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음을 보여준다.\n","\n"],"metadata":{"id":"BbN2w-T5JlCb"}},{"cell_type":"markdown","source":["- **트랜스포머의 입력**\n","  - 트랜스포머의 인코더와 디코더는 단순히 각 단어의 임베딩 벡터들을 입력받는 것이 아니라\n","  - **임베딩 벡터에서 '조정된 값'을 입력**받는다\n"],"metadata":{"id":"oQPDu7zlJk_t"}},{"cell_type":"markdown","source":["# 4.포지셔널 인코딩 (Positional Encoding)"],"metadata":{"id":"YH0pJqkMJk6r"}},{"cell_type":"markdown","source":["- RNN 이 자연어 처리에서 유용했던 이유는?\n","  - 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해\n","  - 각 단어의 **위치 정보(position information)**를 가질 수 있다는 점!!  "],"metadata":{"id":"oEm7Qge2Jk37"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/31379/transformer5_final_final.PNG)\n","- ↑ 위 그림은 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩의 값이 더해지는 것을 보여줍니다.\n","\n"],"metadata":{"id":"ZW5HMCsVJk1M"}},{"cell_type":"markdown","source":["- 임베딩 벡터가 인코더의 입력으로 사용되기 전 포지셔널 인코딩값이 더해지는 과정을 시각화하면 아래와 같습니다.\n","![](https://wikidocs.net/images/page/31379/transformer6_final.PNG)"],"metadata":{"id":"XZMD3pB7Jky0"}},{"cell_type":"markdown","source":["- 포지셔널 인코딩 값들은 어떤 값이기에 '위치 정보'를 반영해줄 수 있는 것일까요? 트랜스포머는 위치 정보를 가진 값을 만들기 위해서 아래의 두 개의 함수를 사용합니다.\n","\n","$$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$$\n","\n","$$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$$"],"metadata":{"id":"YQaW3Mw2Jkwe"}},{"cell_type":"markdown","source":["- 사인 함수와 코사인 함수의 그래프를 상기해보면 요동치는 값의 형태를 생각해볼 수 있다.\n","- 트랜스포머는 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해주므로서 단어의 '순서 정보'를 더하여 준다"],"metadata":{"id":"TAMkItZVJktX"}},{"cell_type":"markdown","source":["- 그런데 위의 두 함수에는 $pos$, $i$, $d_{model}$ 등의 생소한 변수들이 있습니다.\n","- 위의 함수를 이해하기 위해서는 위에서 본 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 **임베딩 벡터가 모여 만들어진 문장 행렬** 과 **포지셔널 인코딩 행렬**의 덧셈 연산을 통해 이루어진다는 점을 이해해야 합니다.\n","\n","![](https://wikidocs.net/images/page/31379/transformer7.PNG)"],"metadata":{"id":"v5WgbHBMJkrB"}},{"cell_type":"markdown","source":["- $pos$ 는 입력 문장에서의 **임베딩 벡터의 위치**,\n","- $i$ 는 임베딩 벡터 내의 **차원의 인덱스**.\n","- 위의 식에 따르면 임베딩 벡터 내의 각 차원의 인덱스 $i$ 가\n","  - 짝수인 경우에는 사인 함수의 값을 사용\n","  - 홀수인 경우에는 코사인 함수의 값을 사용\n","- 즉! 위의 수식에서\n","  - $(pos, 2i)$ 일 때는 사인 함수를 사용!\n","  - $(pos, 2i+1)$일 때는 코사인 함수를 사용!"],"metadata":{"id":"YGXq-JQtJkmB"}},{"cell_type":"markdown","source":["- 위의 식에서 $d_{model}$\n","은 트랜스포머의 **모든 층의 출력 차원**을 의미하는 트랜스포머의 하이퍼파라미터 라고 했습니다.\n","\n","- 앞으로 보게 될 트랜스포머의 각종 구조에서\n","$d_{model}$ 값이 계속해서 등장하는 이유입니다.\n","\n","- 임베딩 벡터 또한 $d_{model}$의 차원을 가지는데 위의 그림에서는 4로 표현되었지만 실제 논문에서는 512의 값을 가집니다."],"metadata":{"id":"2rirV2KLJkjr"}},{"cell_type":"markdown","source":["- 위와 같은 포지셔널 인코딩 방법을 사용하면 **순서 정보**가 보존된다\n","- 예를 들어 각 임베딩 벡터에 포지셔널 인코딩의 값을 더하면 '같은 단어'라고 하더라도 '문장 내의 위치'에 따라서 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라진다.\n","- 이에 따라 **트랜스포머의 입력**은 **순서 정보가 고려된 임베딩 벡터**가 됩니다."],"metadata":{"id":"nSEVQMRsJkg-"}},{"cell_type":"markdown","source":["# 어텐션 (Attention) 세가지"],"metadata":{"id":"ZlRyUAtyPlfe"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/31379/attention.PNG)\n","\n","- **Encoder Self-Attention** 는 '인코더'에서 이루어진다\n","- **Masked Decoder Self-Attention** 과 **Encoder-Decoder Attention**은 '디코더'에서 이루어진다\n","- **XXX Self-Attention** 은 본질적으로 Query, Key, Value 가 '동일'한 출처인 경우를 말합니다.\n","  - 여기서 '동일' 하다는 뜻은 벡터의 값이 같다는 뜻이 아니라, **벡터의 출처가 같다는 의미**!\n","- 반면, 세번째 그림 **Encoder-Decoder Attention** 에서는\n","  - Query가 '디코더의 벡터'인 반면에\n","  - Key와 Value가 '인코더의 벡터'이므로 셀프 어텐션이라고 부르지 않습니다\n","- 정리하면 다음과 같다\n","```\n","인코더의 셀프 어텐션 : Query = Key = Value\n","디코더의 마스크드 셀프 어텐션 : Query = Key = Value\n","디코더의 인코더-디코더 어텐션 : Query : 디코더 벡터 / Key = Value : 인코더 벡터\n","```"],"metadata":{"id":"-bPnEgpJPldR"}},{"cell_type":"markdown","source":["## 트랜스포머 아키텍쳐\n","  - 세가지 어텐션이 각각 어디에서 이루어지는지 보여주는 그림\n","![](https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG)"],"metadata":{"id":"QLvfVeYbPlaa"}},{"cell_type":"code","source":["# ↑ 위 세개의 어텐션에 추가적으로 '멀티헤드(multi-head) 라는 이름이 붙어있다.\n","#  트랜스포머가 어텐션을 병렬적으로 수행"],"metadata":{"id":"6L3YoVYrPlYE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🟦 인코더(Encoder) 의 구조"],"metadata":{"id":"fafgk23EPlTG"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/31379/transformer9_final_ver.PNG)\n","\n","- 트랜스포머는 하이퍼파라미터인 $\\text{num_layers}$\n"," 개수의 인코더 층을 쌓습니다.\n","- 논문에서는 총 6개의 인코더 층을 사용.\n","- 인코더를 하나의 층이라는 개념으로 생각한다면, 하나의 인코더 층은 크게 총 2개의 서브층(sublayer)으로 나뉘어집니다.\n","  - **셀프 어텐션**과 **피드 포워드 신경망**입니다.\n","- 위의 그림에서 '멀티 헤드 셀프 어텐션(Multi-head Self Attention)'과 '포지션 와이즈 피드 포워드 신경망 (Position-wise FFNN)'이라고 적혀있는데, 이는\n","  - **'멀티 헤드' 셀프 어텐션**은 셀프 어텐션을 '병렬적'으로 사용하였다는 의미고,\n","  - **포지션 와이즈 피드 포워드 신경망**은 우리가 알고있는 일반적인 피드 포워드 신경망(FFN)입니다."],"metadata":{"id":"0UyuMbJOPlQQ"}},{"cell_type":"markdown","source":["## 🟡인코더의 셀프 어텐션"],"metadata":{"id":"NwNL2XrFPlN5"}},{"cell_type":"markdown","source":["### 1) 셀프 어텐션의 의미와 이점\n","- (복습) 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구합니다. 그리고 구해낸 이 유사도를 가중치로 하여 키와 맵핑되어있는 각각의 '값(Value)'에 반영해줍니다. 그리고 유사도가 반영된 '값(Value)'을 모두 가중합하여 리턴합니다.\n","\n","![](https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG)"],"metadata":{"id":"L8KRKupFetZL"}},{"cell_type":"markdown","source":["- 여기까지는 앞서 배운 어텐션의 개념입니다. 그런데 어텐션 중에서는 **셀프 어텐션(self-attention)**이라는 것이 있습니다. 어텐션을 자기 자신에게 수행한다는 의미입니다. 앞서 배운 seq2seq에서 어텐션을 사용할 경우의 Q, K, V의 정의를 다시 생각해봅시다.\n","\n","```\n","Q = Query : t 시점의 디코더 셀에서의 은닉 상태\n","K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n","V = Values : 모든 시점의 인코더 셀의 은닉 상태들\n","```\n","- 여기서, **t 시점(타임스텝)** 이라는 것은 계속 변화하면서 반복적으로 쿼리를 수행하므로 결국 **전체 시점**에 대해서 다음과 같이 일반화를 할 수도 있습니다.\n","\n","```\n","Q = Querys : 모든 시점의 디코더 셀에서의 은닉 상태들\n","K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n","V = Values : 모든 시점의 인코더 셀의 은닉 상태들\n","```\n","\n","- 이처럼 기존에는 디코더 셀의 은닉 상태가 Q이고 인코더 셀의 은닉 상태가 K라는 점에서 Q와 K가 서로 다른 값을 가지고 있었습니다.\n","- 그런데 **셀프 어텐션**에서는 Q, K, V가 전부 동일합니다.\n","- 트랜스포머의 셀프 어텐션에서의 Q, K, V는 아래와 같습니다.\n","\n","```\n","Q : 입력 문장의 모든 단어 벡터들\n","K : 입력 문장의 모든 단어 벡터들\n","V : 입력 문장의 모든 단어 벡터들\n","```"],"metadata":{"id":"M8PrR4v9fKuh"}},{"cell_type":"markdown","source":["-  셀프 어텐션을 통해 얻을 수 있는 대표적인 효과 (예시)\n","\n","![](https://wikidocs.net/images/page/31379/transformer10.png)"],"metadata":{"id":"brpBdOqKfKqO"}},{"cell_type":"markdown","source":["### Q, K, V 벡터 얻기"],"metadata":{"id":"rjb5J90MgK0K"}},{"cell_type":"markdown","source":["- 앞서 셀프 어텐션은 입력 문장의 단어 벡터들을 가지고 수행한다고 하였는데,\n","- 사실 셀프 어텐션은\n","  - 인코더의 초기 입력인\n","$d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 셀프 어텐션을 수행하는 것이 아니라!\n","  - 우선 **각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업**을 거칩니다.\n","- 이때! 이 Q벡터, K벡터, V벡터들은\n","  - 초기 입력인 $d_{model}$ 의 차원을 가지는 단어 벡터들보다 '더 작은 차원'을 가진다,\n","- 논문에서는,\n","$d_{model}$=512의 차원을 가졌던 각 단어 벡터들을 64의 차원을 가지는 Q벡터, K벡터, V벡터로 변환하였습니다"],"metadata":{"id":"AyeNO0H-gN2_"}},{"cell_type":"markdown","source":["- 64라는 값은 트랜스포머의 또 다른 하이퍼파라미터인 $\\text{num_heads}$ 로 인해 결정되는데,\n","- 트랜스포머는 $d_{model}$ 을 $\\text{num_heads}$로 나눈 값을 각 Q벡터, K벡터, V벡터의 차원으로 결정합니다.\n","- 논문에서는 $\\text{num_heads}$ 값을 8로 하였습니다. (512 / 8 = 64)"],"metadata":{"id":"Mf1IscbWgNzG"}},{"cell_type":"markdown","source":["- 그림 예시]\n","\n","![](https://wikidocs.net/images/page/31379/transformer11.PNG)\n","\n","- 예를 들어 여기서 사용하고 있는 예문 중 student라는 단어 벡터를 Q, K, V의 벡터로 변환하는 과정을 보겠습니다.\n","<br>\n","\n","- 기존의 벡터로부터 더 작은 벡터는 **'가중치 행렬'**을 곱하므로서 완성됩니다.\n","- 곱해지는 각 '가중치 행렬'의 크기는 $d_{model} \\times (d_{model} / \\text{num_heads})$ 입니다.\n","- 이 가중치 행렬은 훈련 과정에서 '학습'됩니다.\n","  - 즉, 논문과 같이\n","$d_{model}$=512 이고\n","$\\text{num_heads}$=8 라면,\n","  - 각 벡터에 3개의 서로 다른 가중치 행렬을 곱하고 64의 크기를 가지는 Q, K, V 벡터를 얻어냅니다.\n","\n","- 위의 그림은 단어 벡터 중 student 벡터로부터 Q, K, V 벡터를 얻어내는 모습을 보여줍니다. 모든 단어 벡터에 위와 같은 과정을 거치면 I, am, a, student는 각각의 Q, K, V 벡터를 얻습니다."],"metadata":{"id":"Pc5SB402gNQs"}},{"cell_type":"markdown","source":["### 3) 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)\n","- Q, K, V 벡터를 얻었다면 지금부터는 기존에 배운 어텐션 메커니즘과 동일합니다.\n","  - 각 Q벡터는 모든 K벡터에 대해서 **어텐션 스코어**를 구하고, **어텐션 분포**를 구한 뒤에\n","  - 이를 사용하여 모든 V벡터를 가중합하여 **'어텐션 값' 또는 '컨텍스트 벡터'**를 구하게 됩니다.\n","  - 그리고 이를 모든 Q벡터에 대해서 반복합니다."],"metadata":{"id":"6XS9yzYAhk46"}},{"cell_type":"markdown","source":["#### Attention Score 구하기\n","- 이전에, 어텐션 챕터에서 배웠던 어텐션 함수중, 내적만을 사용하는 어텐션 함수 $score(q,k) = q \\cdot k$  가 있었다.\n","- 트랜스포머에서는 여기에 특정값으로 나눠준 어텐션 함수인\n","$score(q,k) = q \\cdot k$ / $\\sqrt{n}$를 사용합니다.\n","- 이러한 함수를 사용하는 어텐션을 어텐션 챕터에서 배운 닷-프로덕트 어텐션(dot-product attention)에서 값을 스케일링하는 것을 추가하였다고 하여 **스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)**이라고 합니다."],"metadata":{"id":"_lE7Flmchw9i"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/31379/transformer13.PNG)\n","<br>\n","- 단어 시퀀스 : \"I\", \"am\", \"a\", \"student\"\n","- 우선 단어 \"I\" 에 대한 Q벡터를 기준으로 설명해보겠습니다.\n","  - ※지금부터 설명하는 과정은\n","    - \"am\"에 대한 Q벡터,\n","    - \"a\"에 대한 Q벡터,\n","    - \"student\"에 대한 Q벡터에 대해서도\n","    - 모두 동일한 과정을 거칩니다.\n","<br>\n","\n","- 위의 그림은 단어 \"I\"에 대한 Q벡터가 모든 K벡터에 대해서 어텐션 스코어를 구하는 것을 보여줍니다.\n","\n","- ※위의 128과 32는 그림 예시에서 임의로 가정한 수치로 신경쓰지 않아도 좋습니다.\n","- 위의 그림에서 어텐션 스코어는 각각 단어 \"I\"가 단어 \"I\", \"am\", \"a\", \"student\"와 얼마나 연관되어 있는지를 보여주는 수치입니다.\n","- 트랜스포머에서는 두 벡터의 내적값을 스케일링하는 값으로 K벡터의 차원을 나타내는 $d_k$에 루트를 씌운 $\\sqrt{d_k}$\n"," 사용하는 것을 택했습니다.\n","- 앞서 언급하였듯이 논문에서 $d_k$ 는 $d_{model} / \\text{num_heads}$ 라는 식에 따라서 64의 값을 가지므로 $\\sqrt{d_k}$는 8의 값을 가집니다."],"metadata":{"id":"qxI-QRuAiI7i"}},{"cell_type":"markdown","source":["#### Attention Value 구하기\n","\n","![](https://wikidocs.net/images/page/31379/transformer14_final.PNG)\n","\n","- 이제 어텐션 스코어에 소프트맥스 함수를 사용하여 어텐션 분포(Attention Distribution)을 구하고, 각 V벡터와 가중합하여 어텐션 값(Attention Value)을 구합니다.\n","- 이를 **단어 \"I\"에 대한 어텐션 값** 또는 **단어 \"I\"에 대한 컨텍스트 벡터(context vector)**라고도 할 수 있습니다.\n","  - am에 대한 Q벡터, a에 대 Q벡터, student에 대한 Q벡터에 대해서도 모두 동일한 과정을 반복하여 각각에 대한 어텐션 값을 구합니다.\n"],"metadata":{"id":"iCAk3NB5i9l8"}},{"cell_type":"markdown","source":["### 4) 행렬 연산으로 일괄 처리"],"metadata":{"id":"s7vsNWNKPlLM"}},{"cell_type":"markdown","source":["-우선, 각 단어 벡터마다 일일히 가중치 행렬을 곱하는 것이 아니라 '문장 행렬'에 '가중치 행렬'을 곱하여 **Q행렬, K행렬, V행렬**을 구합니다.\n","![](https://wikidocs.net/images/page/31379/transformer12.PNG)"],"metadata":{"id":"SChqttEljgCg"}},{"cell_type":"markdown","source":["- 행렬 연산을 통해 **어텐션 스코어**는 어떻게 구할 수 있을까요?\n","- 여기서 Q행렬을 K행렬을 전치$^T$한 행렬과 곱해준다고 해봅시다.\n","- 이렇게 되면 각각의 단어의 Q벡터와 K벡터의 내적이 각 행렬의 원소가 되는 행렬이 결과로 나옵니다.\n","![](https://wikidocs.net/images/page/31379/transformer15.PNG)"],"metadata":{"id":"ADrEgxPnjf-_"}},{"cell_type":"markdown","source":["- 다시 말해 위의 그림의 결과 행렬의 값에 전체적으로\n","$\\sqrt{d_k}$를 나누어주면 이는 각 행과 열이 어텐션 스코어 값을 가지는 행렬이 됩니다.\n","- 예를 들어 \"I\" 행과 \"student\" 열의 값은 \"I\"의 Q벡터와 \"student\"의 K벡터의 어텐션 스코어 값입니다.\n","- 위 행렬을 **'어텐션 스코어 행렬'**이라 합시다.\n","- 어텐션 스코어 행렬을 구하였다면 남은 것은 '어텐션 분포'를 구하고, 이를 사용하여 모든 단어에 대한 '어텐션 값'을 구하는 일입니다.\n","- 이는 간단하게 어텐션 스코어 행렬에 소프트맥스 함수를 사용하고, V행렬을 곱하는 것으로 해결됩니다.\n","- 이렇게 되면 각 단어의 어텐션 값을 모두 가지는 **어텐션 값 행렬 $a$**이 결과로 나옵니다.\n","\n","![](https://wikidocs.net/images/page/31379/transformer16.PNG)\n","\n","- ↑위의 그림은 행렬 연산을 통해 모든 값이 일괄 계산되는 과정을 식으로 보여줍니다.\n"],"metadata":{"id":"57hVQHSDjf6s"}},{"cell_type":"markdown","source":["- 해당 식은 실제 트랜스포머 논문에 기재된 아래의 수식과 정확하게 일치하는 식입니다.\n","\n","$$Attention(Q,K,V) = softmax(\\dfrac{QK^T}{\\sqrt{d_k}})V$$\n","\n","<br>\n","\n","- 위의 행렬 연산에 사용된 행렬의 크기를 모두 정리해봅시다.\n","- 우선 입력 문장의 길이(단어개수)를 $\\text{seq_len}$ 라고 해봅시다.\n","- 그렇다면 문장 행렬의 크기는 $(\\text{seq_len}, d_{model})$\n","입니다.\n","- 여기에 3개의 가중치 행렬을 곱해서 Q, K, V 행렬을 만들어야 합니다.\n","\n","<br>\n","\n","- 우선 행렬의 크기를 정의하기 위해 행렬의 각 행에 해당되는 Q벡터와 K벡터의 차원을 $d_k$라고 하고, V벡터의 차원을 $d_v$\n","라고 해봅시다.\n","- 그렇다면 Q행렬과 K행렬의 크기는\n","$(\\text{seq_len}, d_k)$이며, V행렬의 크기는\n","$(\\text{seq_len}, d_v)$가 되어야 합니다.\n","- 그렇다면 문장 행렬과 Q, K, V 행렬의 크기로부터 가중치 행렬의 크기 추정이 가능합니다.\n","\n","- $W^Q$ 와$W^K$ 는 $(d_{model},d_k)$ 의 크기를 가지며, $W^V$ 는 $(d_{model},d_v)$의 크기를 가집니다.\n","- 단, 논문에서는 $d_k$ 와 $d_v$의 차원은 $d_{model} / \\text{num_heads}$ 와 같습니다.\n","  - 즉! $d_{model} / \\text{num_heads} = d_k = d_v $ 입니다.\n","\n","<br>\n","\n","- 결과적으로 $softmax(\\dfrac{QK^T}{\\sqrt{d_k}})V$ 식을 적용하여 나오는 어텐션 값 $a$행렬\n","의 크기는 $(\\text{seq_len}, d_v)$이 됩니다."],"metadata":{"id":"i8p8ShoekQfR"}},{"cell_type":"markdown","source":["### 6) 멀티 헤드 어텐션(Multi-head Attention)\n","병렬 어텐션!\n","- 앞서 배운 어텐션에서는 $d_{model}$의 차원을 가진 단어 벡터를 $\\text{num_heads}$로 나눈 차원을 가지는 Q, K, V벡터로 바꾸고 어텐션을 수행하였습니다.\n","- 논문 기준으로는 512의 차원의 각 단어 벡터를 8로 나누어 64차원의 Q, K, V 벡터로 바꾸어서 어텐션을 수행한 셈인데,\n","- 이제 $\\text{num_heads}$ 의 의미와, 왜 $d_{model}$\n","의 차원을 가진 단어 벡터를 가지고 어텐션을 하지 않고 차원을 축소시킨 벡터로 어텐션을 수행하였는지 알아보자."],"metadata":{"id":"uhiU1AnikQbX"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/31379/transformer17.PNG)\n","- 트랜스포머 연구진은 한 번의 어텐션을 하는 것보다 **여러번의 어텐션을 병렬로 사용**하는 것이 더 효과적이라고 판단하였습니다.\n","- 그래서 $d_{model}$의 차원을 $\\text{num_heads}$개로 나누어\n","$d_{model} / \\text{num_heads}$의 차원을 가지는 Q, K, V에 대해서 $\\text{num_heads}$개의 **병렬 어텐션**을 수행합니다.\n","- 논문에서는 하이퍼파라미터인 $\\text{num_heads}$의 값을 8로 지정하였고, 8개의 병렬 어텐션이 이루어지게 됩니다. 다시 말해 위에서 설명한 어텐션이 8개로 병렬로 이루어지게 되는데, 이때 각각의 어텐션 값 행렬을 **어텐션 헤드**라고 부릅니다.\n","- 이때 가중치 행렬 $W^Q, W^K, W^V$ 의 값은 8개의 어텐션 헤드마다 전부 다릅니다."],"metadata":{"id":"2zoKAItPjeiy"}},{"cell_type":"markdown","source":["- 병렬어텐션의 효과 : **다른 시각으로 정보들을 수집**할수 있다\n","- 예시]\n","  - 앞서 사용한 예문을 상기해봅니다\n","  \n","  ```\n","  'The animal didn't cross the street, because it was too tired'\n","  '그 동물은 길을 건너지 않았다. 왜냐하면 그것(it)은 너무 피곤하였기 때문이다.'\n","  ```\n","\n","  - 단어 그것(it)이 쿼리였다고 해봅시다.\n","    - 즉, it에 대한 Q벡터로부터 다른 단어와의 연관도를 구하였을 때\n","      - 첫번째 어텐션 헤드는 '그것(it)'과 '동물(animal)'의 연관도를 높게 본다면,\n","      - 두번째 어텐션 헤드는 '그것(it)'과 '피곤하였기 때문이다(tired)'의 연관도를 높게 볼 수 있습니다.\n","      - 각 어텐션 헤드는 전부 다른 시각에서 보고있기 때문입니다."],"metadata":{"id":"cgh0kq-iplf0"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/31379/transformer18_final.PNG)\n","\n","- 병렬 어텐션을 모두 수행하였다면 모든 어텐션 헤드를 연결(concatenate)합니다.\n","- 모두 연결된 어텐션 헤드 행렬의 크기는\n","$(\\text{seq_len}, d_{model})$가 됩니다."],"metadata":{"id":"HqZzK1mkp9kc"}},{"cell_type":"markdown","source":["- 지금까지 그림에서는 학습 지면상의 한계로 '4차원'을\n","$d_{model}$=512로 표현하고, '2차원'을\n","$d_v$=64로 표현해왔기 때문에 위의 그림의 행렬의 크기에 혼동의 있을 수 있으나 8개의 어텐션 헤드의 연결(concatenate) 과정의 이해를 위해 이번 행렬만 예외로 위와 같이 $d_{model}$\n","의 크기를 $d_v$ 의 8배인 16차원으로 표현하였습니다\n","\n","- 아래의 그림에서는 다시 $d_{model}$을 4차원으로 표현합니다.\n","\n","- 어텐션 헤드를 모두 연결한 행렬은 또 다른 가중치 행렬 $W^0$\n","을 곱하게 되는데,\n","  - 이렇게 나온 결과 행렬이 멀티-헤드 어텐션의 최종 결과물입니다.\n","  - 위의 그림은 어텐션 헤드를 모두 연결한 행렬이 가중치 행렬 $W^0$과 곱해지는 과정을 보여줍니다.\n","  - 이때 결과물인 멀티-헤드 어텐션 행렬은 인코더의 입력이었던 문장 행렬의 $(\\text{seq_len}, d_{model})$의 크기와 동일합니다. (어래 그림)\n","\n","![](https://wikidocs.net/images/page/31379/transformer19.PNG)\n","\n"],"metadata":{"id":"cJJDbsL2plbP"}},{"cell_type":"markdown","source":["**인코더의 '입력'크기는 '출력'에서도 유지된다!**\n","\n","- 다시 말해 인코더의 첫번째 서브층인 멀티-헤드 어텐션 단계를 끝마쳤을 때,\n","  - 인코더의 입력으로 들어왔던 행렬의 크기가 아직 유지되고 있음을 기억해둡시다.\n","- 첫번째 서브층인 멀티-헤드 어텐션과 두번째 서브층인 포지션 와이즈 피드 포워드 신경망을 지나면서 인코더의 입력으로 들어올 때의 행렬의 크기는 계속 유지되어야 합니다.\n","- 트랜스포머는 동일한 구조의 인코더를 쌓은 구조입니다.\n","- 논문 기준으로는 인코더가 총 6개입니다.\n","- 인코더에서의 입력의 크기가 출력에서도 동일 크기로 계속 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있습니다."],"metadata":{"id":"4GqUR9OuqybX"}},{"cell_type":"markdown","source":["### 패딩마스크 (padding mask)"],"metadata":{"id":"KMXcTL-0rlgy"}},{"cell_type":"markdown","source":["- 이는 입력 문장에 &lt;PAD&gt; 토큰이 있을 경우 어텐션에서 사실상 제외하기 위한 연산입니다.\n","- 예를 들어 &lt;PAD&gt;가 포함된 입력 문장의 셀프 어텐션의 예제를 봅시다. 이에 대해서 어텐션을 수행하고 어텐션 스코어 행렬을 얻는 과정은 다음과 같습니다.\n","\n","![](https://wikidocs.net/images/page/31379/pad_masking11.PNG)"],"metadata":{"id":"MwLnYDxmruZj"}},{"cell_type":"markdown","source":["- 그런데 사실 단어 &lt;PAD&gt;의 경우에는 실질적인 의미를 가진 단어가 아닙니다.\n","  - 그래서 트랜스포머에서는 Key의 경우에 &lt;PAD&gt; 토큰이 존재한다면 이에 대해서는 유사도를 구하지 않도록 마스킹(Masking)을 해주기로 했습니다.\n","  - 여기서 마스킹이란 어텐션에서 제외하기 위해 값을 가린다는 의미입니다.\n","  - 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query이고, 열에 해당하는 문장은 Key입니다. 그리고 Key에 &lt;PAD&gt;가 있는 경우에는 해당 열 전체를 마스킹을 해줍니다.\n","\n","![](https://wikidocs.net/images/page/31379/pad_masking2.PNG)"],"metadata":{"id":"0WBozax6rlbC"}},{"cell_type":"markdown","source":["- 마스킹을 하는 방법은 어텐션 스코어 행렬의 마스킹 위치에 매우 작은 음수값을 넣어주는 것입니다.\n","- 여기서 매우 작은 음수값이라는 것은 -1,000,000,000과 같은 -무한대에 가까운 수라는 의미입니다.\n","- 현재 어텐션 스코어 함수는 소프트맥스 함수를 지나지 않은 상태입니다. 앞서 배운 연산 순서라면 어텐션 스코어 함수는 소프트맥스 함수를 지나고, 그 후 Value 행렬과 곱해지게 됩니다. 그런데 현재 마스킹 위치에 매우 작은 음수 값이 들어가 있으므로 어텐션 스코어 행렬이 소프트맥스 함수를 지난 후에는 해당 위치의 값은 0이 되어 단어 간 유사도를 구하는 일에 &lt;PAD&gt; 토큰이 반영되지 않게 됩니다.\n","\n","![](https://wikidocs.net/images/page/31379/softmax.PNG)\n","\n","- 위 그림은 소프트맥스 함수를 지난 후를 가정하고 있습니다. 소프트맥스 함수를 지나면 각 행의 어텐션 가중치의 총 합은 1이 되는데, 단어 &lt;PAD&gt;의 경우에는 0이 되어 어떤 유의미한 값을 가지고 있지 않습니다."],"metadata":{"id":"n_5QlPN_sZQJ"}},{"cell_type":"markdown","source":["## 🟡포지션-와이즈 피드 포워드 신경망 (Position-wise FFNN)"],"metadata":{"id":"Q6CwDikSs53_"}},{"cell_type":"markdown","source":["- 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층\n","- FFNN는  완전 연결 FFNN(Fully-connected FFNN) 이다\n","\n","- 포지션 와이즈 FFNN 의 수식\n","\n","$$FFNN(x) = MAX(0, xW_1 + b_1 )W_2 + b_2$$\n","\n","- 식을 그림으로 표현하면 아래와 같다\n","\n","![](https://wikidocs.net/images/page/31379/positionwiseffnn.PNG)\n","\n","\n","- 여기서 $x$는 앞서 멀티 헤드 어텐션의 결과로 나온 $(\\text{seq_len}, d_{model})$의 크기를 가지는 행렬을 말합니다.\n","- 가중치 행렬 $W_1$은 $(d_{model}, d_{ff})$ 의 크기를 가지고\n","- 가중치 행렬 $W_2$ 은 $(d_{ff}, d_{model})$ 의 크기를 가집니다.\n","- 논문에서 은닉층의 크기인 $d_{ff}$는 앞서 하이퍼파라미터를 정의할 때 언급했듯이 2,048의 크기를 가집니다.\n","- 여기서 매개변수 $W_1$, $b_1$, $W_2$, $b_2$ 는 하나의 인코더 층 내에서는 다른 문장, 다른 단어들마다 정확하게 동일하게 사용됩니다.\n","- 하지만 인코더 층마다는 다른 값을 가집니다.\n","\n","![](https://wikidocs.net/images/page/31379/transformer20.PNG)\n","\n","- 위의 그림에서 좌측은 인코더의 입력을 벡터 단위로 봤을 때,\n","  - 각 벡터들이 멀티 헤드 어텐션 층이라는 인코더 내 첫번째 서브 층을 지나 FFNN을 통과하는 것을 보여줍니다.\n","  - 이는 두번째 서브층인 Position-wise FFNN을 의미합니다.\n","- 물론, 실제로는 그림의 우측과 같이 행렬로 연산되는데, 두번째 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(\\text{seq_len}, d_{model})$\n","의 크기가 보존되고 있습니다.\n","  - 하나의 인코더 층을 지난 이 행렬은 다음 인코더 층으로 전달되고, 다음 층에서도 동일한 인코더 연산이 반복됩니다."],"metadata":{"id":"4yuKtp5wtA31"}},{"cell_type":"markdown","source":["## 🟡잔차 연결 (Residual connection) 과 층 정규화 (Layer Normalization)"],"metadata":{"id":"iLCyRcfptAy7"}},{"cell_type":"markdown","source":["![](https://wikidocs.net/images/page/31379/transformer21.PNG)\n","- 앞서, 인코더의 두 개의 서브층에 대해서 이해해보았습니다.\n","- 트랜스포머에서는 이러한 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 기법이 있다\n","- 바로 Add & Norm입니다. 더 정확히는 **잔차 연결(residual connection)**과 **층 정규화(layer normalization)**를 의미합니다."],"metadata":{"id":"rkSresHcs5ys"}},{"cell_type":"markdown","source":["### 1) 잔차연결 (Residual connection)"],"metadata":{"id":"7TuwoS_NvTV1"}},{"cell_type":"markdown","source":["\n","\n","![](https://wikidocs.net/images/page/31379/transformer22.PNG)\n","- 위 그림은 입력 $x$와 $x$에 대한 어떤 함수 $F(x)$의 값을 더한 함수 $H(x)$ 의 구조를 보여줍니다.\n","- 어떤 함수 $F(x)$ 가 트랜스포머에서는 서브층에 해당됩니다. 다시 말해 잔차 연결은 서브층의 '입력'과 '출력'을 더하는 것을 말합니다.\n","- 앞서 언급했듯이 트랜스포머에서 서브층의 입력과 출력은 동일한 차원을 갖고 있으므로, 서브층의 입력과 서브층의 출력은 덧셈 연산을 할 수 있습니다.\n","- 이것이 바로 위의 인코더 그림에서 각 화살표가 서브층의 입력에서 출력으로 향하도록 그려졌던 이유입니다.\n","- 잔차 연결은 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법입니다.\n","- 이를 식으로 표현하면 $x + Sublayer(x)$\n","입니다.\n","\n","- 가령, 서브층이 멀티 헤드 어텐션이었다면 잔차 연결 연산은 다음과 같습니다.\n","\n","$$H(x) = x + Multihead Attention(x)$$\n","\n","- ![](https://wikidocs.net/images/page/31379/residual_connection.PNG)\n","  - 위 그림은 멀티 헤드 어텐션의 입력과 멀티 헤드 어텐션의 결과가 더해지는 과정을 보여줍니다.\n","- 관련논문 : https://arxiv.org/pdf/1512.03385.pdf\n","  - \"Deep Residual Learning for Image Recognition\""],"metadata":{"id":"dRLKG2nEvTKG"}},{"cell_type":"markdown","source":["### 2) 층 정규화(Layer Normalization)\n","- 잔차 연결을 거친 결과는 이어서 **층 정규화** 과정을 거치게됩니다.\n","- 잔차 연결의 입력을 ${x}$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 ${LN}$ 이라고 하였을 때, 잔차 연결 후 층 정규화 연산을 수식으로 표현하자면 다음과 같습니다.\n","\n","$$LN = LayerNorm(x + Sublayer(x))$$\n","\n","- 층 정규화를 하는 과정에 대해서 이해해봅시다. 층 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 돕습니다. 여기서 텐서의 마지막 차원이란 것은 트랜스포머에서는 $d_{model}$\n"," 차원을 의미합니다. 아래 그림은 $d_{model}$\n"," 차원의 방향을 화살표로 표현하였습니다.\n","\n","![](https://wikidocs.net/images/page/31379/layer_norm_new_1_final.PNG)\n","\n","- 층 정규화를 위해서 우선, 화살표 방향으로 각각 평균\n","$\\mu$ 과 분산 $\\sigma^2$ 을 구합니다. 각 화살표 방향의 벡터를 $x_i$ 라고 명명해봅시다.\n","\n","![](https://wikidocs.net/images/page/31379/layer_norm_new_2_final.PNG)\n","\n","- 층 정규화를 수행한 후에는 벡터 $x_i$ 는 $ln_i$ 라는 벡터로 정규화가 됩니다.\n","\n","$$ln_i = LayerNorm(x_i)$$\n"],"metadata":{"id":"9xq9vvV6wBO6"}},{"cell_type":"markdown","source":["- '층 정규화의 수식'을 알아봅시다. 여기서는 층 정규화를 두 가지 과정으로 나누어서 설명하겠습니다.\n","  - 첫번째는 평균과 분산을 통한 정규화,\n","  - 두번째는 감마와 베타를 도입하는 것입니다.\n","- 우선, 평균과 분산을 통해 벡터 $x_i$를 정규화 해줍니다.\n","- $x_i$는 벡터인 반면, 평균 $\\mu_i$ 과 분산 $\\sigma_i^2$은 스칼라입니다.\n","- 벡터 $x_i$의 각 차원을 $k$라고 하였을 때, $x_{ik}$는 다음의 수식과 같이 정규화 할 수 있습니다.\n","- 다시 말해 벡터 $x_i$의 각 $k$차원의 값이 다음과 같이 정규화 되는 것입니다.\n","\n","$$\\hat{x}_{ik}=\\frac{x_{i,k} - \\mu_i}{\\sqrt{\\sigma^2_i + \\epsilon}}$$\n","\n","- $\\epsilon$ (입실론)은 분모가 0이 되는 것을 방지하는 값.\n","- 이제 $\\gamma$ (감마)와  $\\beta$(베타)라는 벡터를 준비합니다. 단, 이들의 초기값은 각각 1과 0입니다.\n","\n","- ![](https://wikidocs.net/images/page/31379/%EA%B0%90%EB%A7%88%EB%B2%A0%ED%83%80.PNG)\n","\n","- $\\gamma$와 $\\beta$ 를 도입한 층 정규화의 최종 수식은 다음과 같으며 $\\gamma$ 와 $\\beta$ 는 학습 가능한 파라미터입니다.\n","\n","$$ln_i = \\gamma \\hat{x}_i + \\beta = LayerNorm(x_i)$$\n","\n","\n","- 관련 논문 : https://arxiv.org/pdf/1607.06450.pdf\n","\n","- 케라스에서는 층 정규화를 위한 **LayerNormalization()**를 제공함"],"metadata":{"id":"nduLTLk5wrQ2"}},{"cell_type":"code","source":[],"metadata":{"id":"ntcO12wNwBKu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🟦 디코더(Decoder) 의 구조"],"metadata":{"id":"bJcY0qfXPlI1"}},{"cell_type":"markdown","source":["인코더에서 디코더로(From Encoder To Decoder)\n","\n","![](https://wikidocs.net/images/page/31379/transformer_from_encoder_to_decoder.PNG)\n","\n","\n","- 위에서 구현된 인코더는 총 $num\\_layers$만큼의\n","층 연산을 순차적으로 한 후에\n","- **마지막 층의 인코더의 출력**을 → 디코더에게 전달합니다.\n","\n","- 인코더 연산이 끝났으면 디코더 연산이 시작되어 디코더 또한 $num\\_layers$만큼의 연산을 하는데,\n","- 이때마다 인코더가 보낸 출력을 → **'각' 디코더 층 연산에 사용**합니다"],"metadata":{"id":"qbAoC-CiPlGL"}},{"cell_type":"markdown","source":["## 디코더의 입력과 문제점\n","![](https://wikidocs.net/images/page/31379/decoder.PNG)\n","\n","- 디코더의 입력\n","  - 디코더도 인코더와 동일하게 '임베딩' 층과 '포지셔널 인코딩'을 거친 후의 **문장 행렬이 입력**된다\n","\n","- 트랜스포머 또한 seq2seq와 마찬가지로 교사 강요(Teacher Forcing)을 사용하여 훈련되므로\n","  - **학습 과정**에서 디코더는 번역할 문장에 해당되는 ```<sos> je suis étudiant```의 문장 행렬을 **'한 번에 입력'**받습니다.\n","\n","  - 그리고 디코더는 이 문장 행렬로부터 각 시점의 단어를 예측하도록 훈련됩니다."],"metadata":{"id":"_bcUHnTQPlDd"}},{"cell_type":"markdown","source":["**여기서 문제점!**\n","\n","- seq2seq의 디코더에 사용되는 **RNN 계열**의 신경망은 입력 단어를 '매 시점마다 순차적'으로 입력받으므로\n","  - 다음 단어 예측에 현재 시점을 포함한 **이전 시점**에 입력된 단어들만 '참고'할 수 있습니다.\n","\n","- 반면, **트랜스포머**는 문장 행렬로 입력을 '한 번'에 받으므로\n","  - 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 '미래 시점(?)'의 단어까지도 참고할 수 있는 현상이 발생합니다.\n","\n","- 가령, ```suis```를 예측해야 하는 시점이라고 해봅시다.\n","  - RNN 계열의 seq2seq의 디코더라면 현재까지\n","디코더에 입력된 단어는 &lt;sos&gt;와 ```je```뿐일 것입니다.\n","  - 반면, 트랜스포머는 '이미' 문장 행렬로 &lt;sos&gt;```je suis étudiant```를 입력받았습니다."],"metadata":{"id":"KMiHeQ_FPlAP"}},{"cell_type":"markdown","source":["## 🟡 디코더의 첫번째 서브층 : 셀프 어텐션과 룩-어헤드 마스크\n","- 이를 위해 트랜스포머의 디코더에서는 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을\n","참고하지 못하도록 **룩-어헤드 마스크(look-ahead mask)**를 도입.\n","직역하면 '미리보기에 대한 마스크'입니다.\n","\n","\n","![](https://wikidocs.net/images/page/31379/%EB%94%94%EC%BD%94%EB%8D%94.PNG)\n","\n","- 룩-어헤드 마스크(look-ahead mask)는 디코더의 첫번째 서브층에서 이루어집니다.\n","\n","- 디코더의 첫번째 서브층인 멀티 헤드 셀프 어텐션 층은 인코더의 첫번째 서브층인 멀티 헤드 셀프 어텐션 층과 동일한 연산을 수행합니다."],"metadata":{"id":"uMicwA4RPk8U"}},{"cell_type":"markdown","source":["- 오직 다른 점은 어텐션 스코어 행렬에서 '마스킹'을 적용한다는 점만 다릅니다.\n","- 우선 다음과 같이 셀프 어텐션을 통해 어텐션 스코어 행렬을 얻습니다.\n","\n","![](https://wikidocs.net/images/page/31379/decoder_attention_score_matrix.PNG)\n","\n","↓이제 자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 다음과 같이 마스킹합니다.\n","\n","![](https://wikidocs.net/images/page/31379/%EB%A3%A9%EC%96%B4%ED%97%A4%EB%93%9C%EB%A7%88%EC%8A%A4%ED%81%AC.PNG)\n","\n","- ↑ 마스킹 된 후의 어텐션 스코어 행렬의 각 행을 보면 '자기 자신과 그 이전 단어들만을 참고'할 수 있음을 볼 수 있습니다.\n","- 그 외에는, 근본적으로 셀프 어텐션이라는 점과, 멀티 헤드 어텐션을 수행한다는 점에서\n","인코더의 첫번째 서브층과 같습니다."],"metadata":{"id":"NyNlOYRV2CO2"}},{"cell_type":"markdown","source":["## 🟡 디코더의 두번째 서브층 : 인코더-디코더 어텐션\n","\n","- 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서는\n","이전의 어텐션들(인코더와 디코더의 첫번째 서브층)과는 공통점이 있으나\n","\n","- 다른점! → 이번에는 **셀프 어텐션이 아닙니다**."],"metadata":{"id":"yscCROH32CKE"}},{"cell_type":"code","source":["# Q (Query) 는 디코더의 행렬\n","# K (key), V (value) 는 인코더의 행렬"],"metadata":{"id":"qEGfimNt2CHR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> 인코더의 첫번째 서브층 : Query = Key = Value\n","\n","> 디코더의 첫번째 서브층 : Query = Key = Value\n","\n","> 디코더의 두번째 서브층 : Query : 디코더 행렬 / Key = Value : 인코더 행렬\n","\n","디코더의 두번째 서브층을 확대해보면, 다음과 같이 인코더로부터 두 개의 화살표가 그려져 있습니다.\n","\n","![](https://wikidocs.net/images/page/31379/%EB%94%94%EC%BD%94%EB%8D%94%EB%91%90%EB%B2%88%EC%A7%B8%EC%84%9C%EB%B8%8C%EC%B8%B5.PNG)\n","\n","- 두 개의 빨간 화살표는 각각 Key와 Value를 의미하며 이는 인코더의 마지막 층에서 온 행렬로부터 얻습니다.\n","\n","- 반면 검정화살표인 Query는 디코더의 첫번째 서브층의 결과 행렬로부터 얻는다는 점이 다릅니다.\n","\n"],"metadata":{"id":"q2VgejLi2CEL"}},{"cell_type":"markdown","source":["- Query가 디코더 행렬, Key가 인코더 행렬일 때,\n","- 어텐션 스코어 행렬을 구하는 과정은 다음과 같습니다.\n","\n","\n","\n","![](https://wikidocs.net/images/page/31379/%EB%94%94%EC%BD%94%EB%8D%94%EB%91%90%EB%B2%88%EC%A7%B8%EC%84%9C%EB%B8%8C%EC%B8%B5%EC%9D%98%EC%96%B4%ED%85%90%EC%85%98%EC%8A%A4%EC%BD%94%EC%96%B4%ED%96%89%EB%A0%AC_final.PNG)"],"metadata":{"id":"aRki8oLL2CAq"}},{"cell_type":"code","source":[],"metadata":{"id":"rdTkSmW_2B8y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🟦트랜스포머 가계도"],"metadata":{"id":"g_C91PY52B4D"}},{"cell_type":"code","source":["# 트랜스포머 모델은 기본적으로 인코더-디코더 구조를 기반으로 하지만\n","# ★★ 인코더와 디코더를 각각 떼어내어 '독립적' 으로 사용할 수도 있다!\n","\n","# 3가지 구조\n","#  인코더 모델\n","#  디코더 모델\n","#  인코더-디코더 모델\n","\n","# 과연 어떤 특징을 가지고 있을까?"],"metadata":{"id":"9Eu1YaTVPk27","executionInfo":{"status":"ok","timestamp":1763468228324,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 인코더와 디코더가 출력하는 값은 결국 각 토큰에 대한 hidden state (은닉 벡터) 다.\n","\n","# 위 3구조의 출력형태가 같기 때문에 , 특정 작업(task)에 따라 적절한 구조를 선택할수 있게 되었다."],"metadata":{"id":"yGh3sPJAJkeH","executionInfo":{"status":"ok","timestamp":1763468338132,"user_tz":-540,"elapsed":4,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 디코더 모델\n","#  타임스텝 마다 하나의 토큰을 생성하는 역할.\n","\n","#  따라서, 어떤 작업이\n","#    시퀀스 데이터를 입력받아 '하나의 결괏값'을 출력해야 한다면\n","#    이 작업은 디코더로 적합하지 않다.\n","\n","#  그러나, 이러한 작업은 인코더의 출력위에 밀집층과 같은 분류를 위한 층을 배치하여 해결해볼수 있다"],"metadata":{"id":"7NJn69euJkbx","executionInfo":{"status":"ok","timestamp":1763468513054,"user_tz":-540,"elapsed":12,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ex) 개체명인식 (NER) -> 인코더를 활용한 분류문제로 다룰수 있다.\n","# 인코더는 두 텍스드의 유사도 측정하는 작업에도 사용 가능."],"metadata":{"id":"nxxO6-iHJkZC","executionInfo":{"status":"ok","timestamp":1763468591831,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 인코더-디코더 모델.\n","#   '요약', '번역', 'QA챗봇' 과 같은 전형적인 sequence-to-sequence 작업에 사용."],"metadata":{"id":"uPFpEE14JkWE","executionInfo":{"status":"ok","timestamp":1763468664411,"user_tz":-540,"elapsed":41,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 디코더만 사용하는 경우는?\n","#"],"metadata":{"id":"8uPZ0bOi7GVE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["출처: https://bit.ly/4g8Byam\n","\n","![](https://pbs.twimg.com/media/GJg3QtMXwAAvUT4?format=jpg&name=900x900)"],"metadata":{"id":"Oy2xPGUk7GSg"}},{"cell_type":"code","source":["# 우측가지는 '디코더 기반' 모델들.\n","#  현재 가장 인기가 많고 수많은 모델들이 만들어지는 중.\n","\n","# 디코더 -> 뛰어난 생성능력 때문이다!\n","#  ex) 텍스트 생성 기능 : 챗봇, 질의 응답, 요약, 번역...\n"],"metadata":{"id":"UIrTXWXU8Y_H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 프롬프트란!"],"metadata":{"id":"Ees0C-eM9c6y"}},{"cell_type":"code","source":["# 인코더의 입력벡터 없이 디코더만으로 생성 가능한가?\n","\n","# 디코더는 이전까지 생성한 텍스트를 입력받아 다음 토큰을 예측하는데..\n","# 이전에 생성한 텍스트, 인코더로부터 입력이 없으면 아무것도 생성 못함.\n","\n","# 그러나!\n","#  마지 이전에 생성한 텍스트인것처럼 어떤 텍스를를 입력해주면?\n","#  인코더의 도움 없어도 다음 토큰을 예측할수 있다.\n","\n","#  이와같이 이전에 생성한 텍스트 인것처럼 디코더에 전달하는 초기텍스트를?\n","#   => 프롬프트 prompt!"],"metadata":{"id":"2L0Oswzl8Y6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 위 그림에서\n","#  보라색으로 채워진 모델 -> 오픈소스 모델\n","#  희색 바탕의 모델  -> 클로즈드 소스  (상업 모델.2)"],"metadata":{"id":"2YyBDzHS9syB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 전이학습 (transfer learning)"],"metadata":{"id":"fsec-ynt8Y4B"}},{"cell_type":"code","source":["# 오프소스 LLM 이 인기가 높아지는 이유\n","#  - 모델구조 공개\n","#  - 대규모 데이터로 학습된 파라미터 공개\n","\n","# 누구나 이를 사용하여 현재 주어진 작업에 맞게 접목하여 사용할수 있다.\n","#  이러한 방법을 전이 학습이라 함."],"metadata":{"id":"aZWdHIeW8Y1T","executionInfo":{"status":"ok","timestamp":1763469790289,"user_tz":-540,"elapsed":46,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# 전이학습은\n","\n","# 사전훈련된 모델을 새로운 작업에 맞춰\n","# '재사용' 하거나 '미세 조정(fine-tuning)' 하여 사용하는 방법\n"],"metadata":{"id":"u7yplvwN8Yyj","executionInfo":{"status":"ok","timestamp":1763469797618,"user_tz":-540,"elapsed":2,"user":{"displayName":"Sam Coding","userId":"14578619261721572933"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 전이학습의 이점\n","#   시간과 비용 절약.  사전학습모델 재사용.\n","\n","# 트랜스포머 모델이 인기 있는 이유도 '전이학습이 가능하기 때문'!"],"metadata":{"id":"ISswzXE-8Yv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xEdlc5oT8Ytl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7tO5gpCe8YqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"L-Dm82qS8YnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pSW2nOhc8Yjd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tMeMARzW7GO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmLQrgGtJgJO"},"outputs":[],"source":[]}]}